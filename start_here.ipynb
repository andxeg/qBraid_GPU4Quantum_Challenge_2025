{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Welcome 2025 qBraid GPU4Quantum Challenge Submission\n",
    "\n",
    "Greetings from the **Qvengers Team**! üí•\n",
    "\n",
    "**Members:**\n",
    "- Andrew Maciejunes\n",
    "- Marlon Jost  \n",
    "- Andrei Chupakhin\n",
    "- Pranik Chainani  \n",
    "- Vlad Gaylun\n",
    "\n",
    "For more details about challenge and our solution see [README.md](./README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 1: Install dependencies\n",
    "\n",
    "Before running any code, please install the required Python packages.\n",
    "\n",
    "Open a **terminal** in the left sidebar (or add a new code cell below) and run:\n",
    "\n",
    "```bash\n",
    "$ bash postStart.sh\n",
    "\n",
    "```\n",
    "\n",
    "**NOTE!** This might take up to 5 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash postStart.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 2: You're ready!\n",
    "\n",
    "Once dependencies are installed, you're ready to:\n",
    "\n",
    "- Run training scripts or notebooks\n",
    "- Modify graph or quantum circuit code\n",
    "- Track experiments using W&B (Weights & Biases)\n",
    "- Have fun exploring quantum ML!\n",
    "\n",
    "If you need help, check the `README.md` or explore other notebooks in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Step 3: Let's dive deep!\n",
    "\n",
    "You can open these files and notebooks manually from the left sidebar, or click below if running inside qBraid Lab:\n",
    "\n",
    "1. GPT vs QOkit benchmark:\n",
    "    - Open terminal, change directory to `benchmarks` and run `benchmark_A.py` script:\n",
    "        ```bash\n",
    "            $ cd ./benchmarks\n",
    "            $ python ./benchmark_A.py\n",
    "        ```\n",
    "    - Open and run [./benchmarks/process_results.ipynb](./benchmarks/process_results.ipynb) to process results for benchmark\n",
    "    - Make sure you specify correct parameters inside [./benchmarks/benchmark_A.py](./benchmarks/benchmark_A.py). Example:\n",
    "        - Number of assets: `10`\n",
    "        - Number of assets to select: `2`\n",
    "        - Risk aversion parameter: `0.6`\n",
    "        - Number of trials: `3`\n",
    "    - Make sure all checkpoints were downloaded from GitHub repositories, details see in `README.md`\n",
    "2. Decomposition pipeline\n",
    "    - Open Jupyter Notebook [./benchmarks/decomposition_pipeline.ipynb](./benchmarks/decomposition_pipeline.ipynb) and run all cells\n",
    "    - This notebook executes a sophisticated **Binary Markowitz portfolio optimization** for a set of **150 Nasdaq assets** over a 10-year period. It first prepares the data by **denoising the covariance matrix** using Random Matrix Theory (RMT) to remove market noise, and then applies **Spectral Clustering** to group the assets into 10 similarity clusters for diversification. The core process involves solving a **Mixed Integer Linear Program (MILP)** to select an optimal 30-asset portfolio while ensuring a minimum of three assets are picked from each cluster. The solution is found in two ways: a computationally expensive **Global Optimization** for the theoretical optimum, and a **Pipeline Optimization** that unions the results of 10 smaller, faster cluster-specific optimizations. Finally, the notebook compares the two optimized portfolios, demonstrating that the efficient pipeline approach yields a high-quality solution with an **approximation ratio close to 1.0**.\n",
    "4. Partitioning with GPT\n",
    "    - Open Jupyter Notebook [./benchmarks/run_partitioning_GPT.ipynb](./benchmarks/run_partitioning_GPT.ipynb) and run all cells\n",
    "    - That notebook executed a partitioning-based portfolio optimization experiment, aiming to select 30 optimal assets from a set of 150. The process successfully prepared the data and partitioned the assets into 27 distinct groups to facilitate sub-optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [Default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f4a5cafb3a6059f71abfe02f003db547cc659036b80a5f94b4cfba1d186d1f3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
