{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference code for trained nanoGPT\n",
    "\n",
    "See details in GPT-QAOA paper: https://arxiv.org/pdf/2504.16350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Levenshtein in /opt/conda/lib/python3.11/site-packages (0.27.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (3.5)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: qiskit in /opt/conda/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /opt/conda/lib/python3.11/site-packages (from Levenshtein) (3.13.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: rustworkx>=0.15.0 in /opt/conda/lib/python3.11/site-packages (from qiskit) (0.16.0)\n",
      "Requirement already satisfied: numpy<3,>=1.17 in /opt/conda/lib/python3.11/site-packages (from qiskit) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/conda/lib/python3.11/site-packages (from qiskit) (1.16.0)\n",
      "Requirement already satisfied: dill>=0.3 in /opt/conda/lib/python3.11/site-packages (from qiskit) (0.3.8)\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from qiskit) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from qiskit) (4.13.2)\n",
      "Requirement already satisfied: pbr>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit) (6.1.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from pbr>=2.0.0->stevedore>=3.0.0->qiskit) (69.5.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Levenshtein networkx nltk qiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
      "\u001b[0mFiles removed: 0 (0 bytes)\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28\n",
      "drwxr-xr-x  7 jovyan users 4096 Jul  7 12:57 .\n",
      "drwxrwxrwx 19 jovyan users 4096 Jul 11 14:16 ..\n",
      "drwxr-xr-x  2 jovyan users 4096 Jul  7 02:31 fontconfig\n",
      "drwxr-xr-x  3 jovyan users 4096 Jul  7 02:19 jedi\n",
      "drwxr-xr-x  2 jovyan users 4096 Jul  7 02:31 matplotlib\n",
      "drwxr-xr-x  4 jovyan users 4096 Jul  7 12:59 pip\n",
      "drwxr-xr-x  3 jovyan users 4096 Jul  7 02:33 wandb\n"
     ]
    }
   ],
   "source": [
    "!ls -la ~/.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t/home/jovyan/.cache/fontconfig\n",
      "378\t/home/jovyan/.cache/jedi\n",
      "1\t/home/jovyan/.cache/matplotlib\n",
      "3\t/home/jovyan/.cache/pip\n",
      "1\t/home/jovyan/.cache/wandb\n"
     ]
    }
   ],
   "source": [
    "!du -sm ~/.cache/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "import importlib\n",
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import Levenshtein\n",
    "import networkx\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "import qiskit\n",
    "from qiskit import qpy\n",
    "from qiskit import QuantumCircuit\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# import model_qaoa\n",
    "import model_qaoa_cached\n",
    "\n",
    "# importlib.reload(model_qaoa)\n",
    "importlib.reload(model_qaoa_cached)\n",
    "# from model_qaoa import GPT, GPTConfig\n",
    "from model_qaoa_cached import GPT, GPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Must be called early, before model creation or torch.compile\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graphs(file: pathlib.Path) -> List[networkx.classes.graph.Graph]:\n",
    "    with open(file, \"rb\") as f:\n",
    "        graphs = pickle.load(f)\n",
    "    return graphs\n",
    "\n",
    "\n",
    "def load_circuits(file: pathlib.Path) -> List[qiskit.circuit.quantumcircuit.QuantumCircuit]:\n",
    "    with open(file, \"rb\") as f:\n",
    "        circuits = qpy.load(f)\n",
    "    return circuits\n",
    "\n",
    "\n",
    "def graph_to_str_v1(graph: networkx.classes.graph.Graph) -> str:\n",
    "    g_nodes_str = \" \".join([f\"{graph.nodes[u]['mu']:.2f}\" for u in graph.nodes]) # 2-decimal precision\n",
    "    g_edges_str = \", \".join([f\"({u}, {v}), {graph.edges[u, v]['weight']:.2f}\" for u, v in graph.edges]) # 2-decimal precision\n",
    "    bos_token = \"<bos>\"\n",
    "    end_of_graph_token = \"<end_of_graph>\"\n",
    "    nodes_weight_start = \"<node_weights_start>\"\n",
    "    nodes_weight_end = \"<node_weights_end>\"\n",
    "    return \" \".join([\n",
    "        bos_token,\n",
    "        \"<format_v1>\",\n",
    "        nodes_weight_start,\n",
    "        g_nodes_str,\n",
    "        nodes_weight_end,\n",
    "        g_edges_str,\n",
    "        end_of_graph_token\n",
    "    ])\n",
    "\n",
    "\n",
    "def graph_to_str_v2(graph: networkx.classes.graph.Graph) -> str:\n",
    "    g_nodes_str = \" \".join([f\"{graph.nodes[u]['mu']:.3f}\" for u in graph.nodes]) # 3-decimal precision\n",
    "    g_edges_str = \", \".join([f\"({u}, {v}), {graph.edges[u, v]['weight']:.3f}\" for u, v in graph.edges]) # 3-decimal precision\n",
    "    bos_token = \"<bos>\"\n",
    "    end_of_graph_token = \"<end_of_graph>\"\n",
    "    nodes_weight_start = \"<node_weights_start>\"\n",
    "    nodes_weight_end = \"<node_weights_end>\"\n",
    "    return \" \".join([\n",
    "        bos_token,\n",
    "        \"<format_v2>\",\n",
    "        nodes_weight_start,\n",
    "        g_nodes_str,\n",
    "        nodes_weight_end,\n",
    "        g_edges_str,\n",
    "        end_of_graph_token\n",
    "    ])\n",
    "\n",
    "\n",
    "def graph_to_tokens_old_format(graph: networkx.classes.graph.Graph) -> List[str]:\n",
    "    \"\"\"\n",
    "        Compound tokens like '(0, 1)' and '[0 1]'.\n",
    "        And 2-decimal precision for float numbers.\n",
    "        Graphs only with edges weights.\n",
    "    \"\"\"\n",
    "    bos_token = \"<bos>\"\n",
    "    end_of_graph_token = \"<end_of_graph>\"\n",
    "    graph_tokens = []\n",
    "    graph_tokens.append(bos_token)\n",
    "    for u, v in graph.edges:\n",
    "        graph_tokens.append(f\"({u},{v})\")\n",
    "        graph_tokens.append(f\"{graph.edges[u, v]['weight']:.2f}\") # 2-decimal precision\n",
    "    graph_tokens.append(end_of_graph_token)    \n",
    "    return graph_tokens\n",
    "\n",
    "\n",
    "def graph_to_tokens_v1(graph: networkx.classes.graph.Graph) -> List[str]:\n",
    "    \"\"\"\n",
    "        Compound tokens like '(0, 1)' and '[0 1]'.\n",
    "        And 2-decimal precision for float numbers.\n",
    "        Graphs with nodes and edges weights.\n",
    "    \"\"\"\n",
    "    bos_token = \"<bos>\"\n",
    "    end_of_graph_token = \"<end_of_graph>\"\n",
    "    nodes_weight_start = \"<node_weights_start>\"\n",
    "    nodes_weight_end = \"<node_weights_end>\"\n",
    "    graph_tokens = []\n",
    "    graph_tokens.append(bos_token)\n",
    "    graph_tokens.append(\"<format_v1>\")\n",
    "\n",
    "    # nodes\n",
    "    graph_tokens.append(nodes_weight_start)\n",
    "    for u in graph.nodes:\n",
    "        if \"return_\" in graph.nodes[u]:\n",
    "            graph_tokens.append(f\"{graph.nodes[u]['return_']:.2f}\") # 2-decimal precision\n",
    "        elif \"mu\" in graph.nodes[u]:\n",
    "            graph_tokens.append(f\"{graph.nodes[u]['mu']:.2f}\") # 2-decimal precision\n",
    "        else:\n",
    "            raise Exception(f\"Cannot find return value using key 'mu' and 'return_'.\")\n",
    "    \n",
    "    graph_tokens.append(nodes_weight_end)\n",
    "    \n",
    "    # edges\n",
    "    for u, v in graph.edges:\n",
    "        graph_tokens.append(f\"({u},{v})\")\n",
    "        graph_tokens.append(f\"{graph.edges[u, v]['weight']:.2f}\") # 2-decimal precision\n",
    "    graph_tokens.append(end_of_graph_token)    \n",
    "    return graph_tokens\n",
    "\n",
    "\n",
    "def graph_to_tokens_v2(graph: networkx.classes.graph.Graph) -> List[str]:\n",
    "    \"\"\"\n",
    "        Compositional tokens like '(', '0', '1', ')'.\n",
    "        And 3-decimal precision for float numbers.\n",
    "        Graphs with nodes and edges weights.\n",
    "    \"\"\"\n",
    "    bos_token = \"<bos>\"\n",
    "    end_of_graph_token = \"<end_of_graph>\"\n",
    "    nodes_weight_start = \"<node_weights_start>\"\n",
    "    nodes_weight_end = \"<node_weights_end>\"\n",
    "    graph_tokens = []\n",
    "    graph_tokens.append(bos_token)\n",
    "    graph_tokens.append(\"<format_v2>\")\n",
    "    \n",
    "    # nodes\n",
    "    graph_tokens.append(nodes_weight_start)\n",
    "    for u in graph.nodes:\n",
    "        graph_tokens.append(f\"{graph.nodes[u]['mu']:.3f}\") # 3-decimal precision\n",
    "    graph_tokens.append(nodes_weight_end)\n",
    "    \n",
    "    # edges\n",
    "    for u, v in graph.edges:\n",
    "        graph_tokens.append(\"(\")\n",
    "        graph_tokens.append(f\"{u}\")\n",
    "        graph_tokens.append(f\"{v}\")\n",
    "        graph_tokens.append(\")\")\n",
    "        graph_tokens.append(f\"{graph.edges[u, v]['weight']:.3f}\") # 3-decimal precision\n",
    "    graph_tokens.append(end_of_graph_token)\n",
    "    return graph_tokens\n",
    "\n",
    "\n",
    "def circuit_to_str_v1(circuit: qiskit.circuit.quantumcircuit.QuantumCircuit) -> str:\n",
    "    op_names_exclude = {\"barrier\", \"h\", \"initialize\", \"measure\"}\n",
    "\n",
    "    circuit_str = \"\"\n",
    "    for idx, instruction in enumerate(circuit.data):\n",
    "        op_name = instruction.operation.name\n",
    "        if op_name in op_names_exclude: # skip unnecessary operations\n",
    "            continue\n",
    "        params = instruction.operation.params\n",
    "        params_str = \"\".join(map(lambda x: f\"{x:.2f}\", params))\n",
    "        qubit_labels = [circuit.qubits.index(qubit) for qubit in instruction.qubits]\n",
    "        circuit_str += f\"<new_layer_p>, {op_name}, {qubit_labels}\" + (f\", {params_str}\" if params_str else \"\") + \" \"\n",
    "    \n",
    "    return circuit_str\n",
    "\n",
    "\n",
    "def circuit_to_str_v2(circuit: qiskit.circuit.quantumcircuit.QuantumCircuit) -> str:\n",
    "    op_names_exclude = {\"barrier\", \"h\", \"initialize\", \"measure\"}\n",
    "\n",
    "    circuit_str = \"\"\n",
    "    for idx, instruction in enumerate(circuit.data):\n",
    "        op_name = instruction.operation.name\n",
    "        if op_name in op_names_exclude: # skip unnecessary operations\n",
    "            continue\n",
    "        params = instruction.operation.params\n",
    "        params_str = \"\".join(map(lambda x: f\"{x:.2f}\", params)) # 2-decimal precision\n",
    "        qubit_labels = [circuit.qubits.index(qubit) for qubit in instruction.qubits]\n",
    "        qubit_labels_str = \"(\" + \", \".join(map(str, qubit_labels)) + \")\"\n",
    "        circuit_str += f\"<new_layer_p>, {op_name}, {qubit_labels_str}\" + (f\", {params_str}\" if params_str else \"\") + \" \"\n",
    "    \n",
    "    return circuit_str\n",
    "\n",
    "\n",
    "def circuit_to_tokens_old_format(circuit: QuantumCircuit) -> List[str]:\n",
    "    return circuit_to_tokens_v1(circuit)\n",
    "\n",
    "\n",
    "def circuit_to_tokens_v1(circuit: qiskit.circuit.quantumcircuit.QuantumCircuit) -> List[str]:\n",
    "    \"\"\"\n",
    "        Compound tokens like '(0, 1)' and '[0 1]'.\n",
    "        And 2-decimal precision for float numbers.\n",
    "    \"\"\"\n",
    "    op_names_exclude = {\"barrier\", \"h\", \"initialize\", \"measure\"}\n",
    "    end_of_circuit_token = \"<end_of_circuit>\"\n",
    "\n",
    "    circuit_tokens = []\n",
    "    for idx, instruction in enumerate(circuit.data):\n",
    "        op_name = instruction.operation.name\n",
    "        if op_name in op_names_exclude: # skip unnecessary operations\n",
    "            continue\n",
    "        params = instruction.operation.params\n",
    "        qubit_labels = [circuit.qubits.index(qubit) for qubit in instruction.qubits]\n",
    "        layer = []\n",
    "        layer.append(\"<new_layer_p>\")\n",
    "        layer.append(f\"{op_name}\")\n",
    "        layer.append(f\"{qubit_labels}\") # compound token for qubit labels\n",
    "        if params:\n",
    "            params_with_spaces = [f\"{params[i % 2]:.2f}\" if i % 2 == 0 else \" \" for i in range(len(params) * 2 - 1)] # 2-decimal precision\n",
    "            layer.extend(params_with_spaces)\n",
    "        circuit_tokens.extend(layer)\n",
    "    circuit_tokens.append(end_of_circuit_token)\n",
    "    return circuit_tokens\n",
    "\n",
    "\n",
    "def circuit_to_tokens_v2(circuit: qiskit.circuit.quantumcircuit.QuantumCircuit) -> List[str]:\n",
    "    \"\"\"\n",
    "        Compositional tokens like '(', '0', '1', ')'.\n",
    "        And 2-decimal precision for float numbers.\n",
    "    \"\"\"\n",
    "    op_names_exclude = {\"barrier\", \"h\", \"initialize\", \"measure\"}\n",
    "    end_of_circuit_token = \"<end_of_circuit>\"\n",
    "\n",
    "    circuit_tokens = []\n",
    "    for idx, instruction in enumerate(circuit.data):\n",
    "        op_name = instruction.operation.name\n",
    "        if op_name in op_names_exclude: # skip unnecessary operations\n",
    "            continue\n",
    "        params = instruction.operation.params\n",
    "        qubit_labels = [circuit.qubits.index(qubit) for qubit in instruction.qubits]\n",
    "        layer = []\n",
    "        layer.append(\"<new_layer_p>\")\n",
    "        layer.append(f\"{op_name}\")\n",
    "\n",
    "        # compositional tokens for qubit labels\n",
    "        layer.append(\"(\")\n",
    "        for l in qubit_labels:\n",
    "            layer.append(f\"{l}\")\n",
    "        layer.append(\")\")\n",
    "        \n",
    "        if params:\n",
    "            params_with_spaces = [f\"{params[i % 2]:.2f}\" if i % 2 == 0 else \" \" for i in range(len(params) * 2 - 1)] # 2-decimal precision\n",
    "            layer.extend(params_with_spaces)\n",
    "        circuit_tokens.extend(layer)\n",
    "    circuit_tokens.append(end_of_circuit_token)\n",
    "    return circuit_tokens\n",
    "\n",
    "\n",
    "def generate_long_circuit(model, graph_tokens, stoi, itos, max_total_tokens=6000, temperature=1.0, top_k=10):\n",
    "    model.eval()\n",
    "    pad_id = model.config.pad_token_id\n",
    "    stop_id = model.config.stop_token_id\n",
    "    block_size = model.config.block_size\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # === Encode input graph ===\n",
    "    input_ids = [stoi.get(tok, stoi[\"<unk>\"]) for tok in graph_tokens]\n",
    "    generated_ids = input_ids[:]\n",
    "\n",
    "    while len(generated_ids) < max_total_tokens:\n",
    "        # Prepare input window: last block_size tokens\n",
    "        idx_cond = torch.tensor([generated_ids[-block_size:]], dtype=torch.long).to(device)\n",
    "        if (idx_cond == pad_id).all():\n",
    "            print(\"Context is only padding. Stopping early.\")\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # logits[:, pad_id] = -float('Inf')  # prevent sampling <pad>\n",
    "\n",
    "            # if top_k is not None:\n",
    "            #     v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            #     logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            if top_k is not None:\n",
    "                top_k = min(top_k, logits.size(-1))\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "\n",
    "                # Apply top-k mask, but fallback if all logits would be -inf\n",
    "                threshold = v[:, [-1]]  # shape: (B, 1)\n",
    "                logits_masked = logits.clone()\n",
    "                logits_masked[logits < threshold] = -float(\"Inf\")\n",
    "\n",
    "                if torch.isinf(logits_masked).all():\n",
    "                    # fallback: don't mask at all\n",
    "                    print(\"All logits would be masked. Falling back to unfiltered logits.\")\n",
    "                else:\n",
    "                    logits = logits_masked\n",
    "\n",
    "            if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "                print(\"NaN or Inf detected in logits. Dumping logits:\")\n",
    "                print(logits)\n",
    "                raise RuntimeError(\"NaNs or Infs in logits\")\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            if torch.isnan(probs).any() or torch.isinf(probs).any():\n",
    "                raise RuntimeError(\"Invalid probabilities: contains NaN or Inf after softmax.\")\n",
    "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        generated_ids.append(next_id)\n",
    "\n",
    "        # Stop if <end_of_circuit> is generated\n",
    "        if stop_id is not None and next_id == stop_id:\n",
    "            break\n",
    "\n",
    "    # === Decode circuit tokens (excluding graph) ===\n",
    "    generated_tokens = [itos[i] for i in generated_ids]\n",
    "    for stop_token in (\"<end_of_circuit>\", \"<pad>\"):\n",
    "        if stop_token in generated_tokens:\n",
    "            generated_tokens = generated_tokens[:generated_tokens.index(stop_token)]\n",
    "    generated_circuit_tokens = generated_tokens[len(input_ids):]\n",
    "    return generated_circuit_tokens\n",
    "\n",
    "\n",
    "def generate_long_circuit_compiled(\n",
    "    model,\n",
    "    config,\n",
    "    graph_tokens,\n",
    "    stoi,\n",
    "    itos,\n",
    "    max_total_tokens=6000,\n",
    "    temperature=1.0,\n",
    "    top_k=10\n",
    "):\n",
    "    pad_id = config.pad_token_id\n",
    "    stop_id = config.stop_token_id\n",
    "    block_size = config.block_size\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    print(f\"device: {device}\")\n",
    "\n",
    "    # Encode input graph\n",
    "    input_ids = [stoi.get(tok, stoi[\"<unk>\"]) for tok in graph_tokens]\n",
    "    generated_ids = input_ids[:]\n",
    "\n",
    "    while len(generated_ids) < max_total_tokens:\n",
    "        # Take last `block_size` tokens as context\n",
    "        idx_cond = torch.tensor([generated_ids[-block_size:]], dtype=torch.long, device=device)\n",
    "\n",
    "        # Optional: stop early if all pad tokens (unlikely with real data)\n",
    "        if torch.all(idx_cond == pad_id):\n",
    "            break\n",
    "\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :]  # only last token\n",
    "        logits /= temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            k = min(top_k, logits.size(-1))\n",
    "            v, _ = torch.topk(logits, k)\n",
    "            threshold = v[:, [-1]]\n",
    "            logits_masked = logits.clone()\n",
    "            logits_masked[logits < threshold] = -float(\"Inf\")\n",
    "\n",
    "            # Avoid fallback logic for performance; assume model is healthy\n",
    "            logits = logits_masked\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)[0, 0].item()\n",
    "        generated_ids.append(next_id)\n",
    "\n",
    "        # Stop if end token generated\n",
    "        if stop_id is not None and next_id == stop_id:\n",
    "            break\n",
    "\n",
    "    # Decode tokens, skip graph\n",
    "    generated_tokens = [itos[i] for i in generated_ids]\n",
    "    for stop_token in (\"<end_of_circuit>\", \"<pad>\"):\n",
    "        if stop_token in generated_tokens:\n",
    "            generated_tokens = generated_tokens[:generated_tokens.index(stop_token)]\n",
    "\n",
    "    return generated_tokens[len(input_ids):]\n",
    "\n",
    "\n",
    "def generate_long_circuit_compiled_cached(\n",
    "    model,\n",
    "    config,\n",
    "    graph_tokens,\n",
    "    stoi,\n",
    "    itos,\n",
    "    max_total_tokens=6000,\n",
    "    temperature=1.0,\n",
    "    top_k=10\n",
    "):\n",
    "    pad_id = config.pad_token_id\n",
    "    stop_id = config.stop_token_id\n",
    "    block_size = config.block_size\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    print(f\"device: {device}\")\n",
    "\n",
    "    # Encode input graph\n",
    "    input_ids = [stoi.get(tok, stoi[\"<unk>\"]) for tok in graph_tokens]\n",
    "    generated_ids = input_ids[:]\n",
    "\n",
    "    # Track state across tokens\n",
    "    past_key_values = None\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long, device=device)\n",
    "\n",
    "    # Prime the cache with the full input graph\n",
    "    with torch.no_grad():\n",
    "        logits, _, past_key_values = model(input_tensor, use_cache=True)\n",
    "\n",
    "    while len(generated_ids) < max_total_tokens:\n",
    "        idx_cond = torch.tensor([[generated_ids[-1]]], dtype=torch.long, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, _, past_key_values = model(\n",
    "                idx_cond, past_key_values=past_key_values, use_cache=True\n",
    "            )\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)[0, 0].item()\n",
    "\n",
    "        generated_ids.append(next_id)\n",
    "\n",
    "        if stop_id is not None and next_id == stop_id:\n",
    "            break\n",
    "\n",
    "    # === Decode output ===\n",
    "    generated_tokens = [itos[i] for i in generated_ids]\n",
    "    for stop_token in (\"<end_of_circuit>\", \"<pad>\"):\n",
    "        if stop_token in generated_tokens:\n",
    "            generated_tokens = generated_tokens[:generated_tokens.index(stop_token)]\n",
    "\n",
    "    return generated_tokens[len(input_ids):]\n",
    "\n",
    "\n",
    "# make sure:\n",
    "# - Ensure all tensors are created on CPU.\n",
    "# - Ensure all tensors are created on CPU.\n",
    "# - Make sure model.eval() is called after transferring the model to CPU.\n",
    "def generate_long_circuit_cpu(\n",
    "    model,\n",
    "    config,\n",
    "    graph_tokens,\n",
    "    stoi,\n",
    "    itos,\n",
    "    max_total_tokens=6000,\n",
    "    temperature=1.0,\n",
    "    top_k=10\n",
    "):\n",
    "    pad_id = config.pad_token_id\n",
    "    stop_id = config.stop_token_id\n",
    "    block_size = config.block_size\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"device: {device}\")\n",
    "\n",
    "    # Encode input graph\n",
    "    input_ids = [stoi.get(tok, stoi[\"<unk>\"]) for tok in graph_tokens]\n",
    "    generated_ids = input_ids[:]\n",
    "\n",
    "    # Track state across tokens\n",
    "    past_key_values = None\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long, device=device)\n",
    "\n",
    "    # Prime the cache with the full input graph\n",
    "    with torch.no_grad():\n",
    "        logits, _, past_key_values = model(input_tensor, use_cache=True)\n",
    "\n",
    "    while len(generated_ids) < max_total_tokens:\n",
    "        idx_cond = torch.tensor([[generated_ids[-1]]], dtype=torch.long, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, _, past_key_values = model(\n",
    "                idx_cond, past_key_values=past_key_values, use_cache=True\n",
    "            )\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)[0, 0].item()\n",
    "\n",
    "        generated_ids.append(next_id)\n",
    "\n",
    "        if stop_id is not None and next_id == stop_id:\n",
    "            break\n",
    "\n",
    "    # === Decode output ===\n",
    "    generated_tokens = [itos[i] for i in generated_ids]\n",
    "    for stop_token in (\"<end_of_circuit>\", \"<pad>\"):\n",
    "        if stop_token in generated_tokens:\n",
    "            generated_tokens = generated_tokens[:generated_tokens.index(stop_token)]\n",
    "\n",
    "    return generated_tokens[len(input_ids):]\n",
    "\n",
    "\n",
    "def warmup_model(model, input_len: int, device: str = \"cuda\"):\n",
    "    \"\"\"Run a warm-up pass with fixed input length to compile the graph once.\"\"\"\n",
    "    dummy_input = torch.randint(0, model.config.vocab_size, (1, input_len), device=device)\n",
    "    with torch.inference_mode():\n",
    "        _ = model(dummy_input, use_cache=True)\n",
    "\n",
    "\n",
    "def generate_batch(graphs_batch, model, gptconf, stoi, itos, max_total_tokens, device):\n",
    "    if device != \"cuda\":\n",
    "        print(\"Device should be equal to 'cuda'\")\n",
    "        return None, None\n",
    "\n",
    "    print(\"Warm-up model\")\n",
    "    start_t = time.time()\n",
    "    warmup_model(model, input_len=len(graph_to_tokens(graphs_batch[0])), device=device)\n",
    "    end_t = time.time()\n",
    "    print(f\"Elapsed time for warm-up: {end_t - start_t} secs\")\n",
    "\n",
    "    times = []\n",
    "    results = []\n",
    "    for graph in tqdm.tqdm(graphs_batch):\n",
    "        graph_tokens = graph_to_tokens(graph)\n",
    "        start_t = time.time()\n",
    "        generated_circuit_tokens = generate_long_circuit_compiled_cached(\n",
    "            model,\n",
    "            gptconf,\n",
    "            graph_tokens,\n",
    "            stoi,\n",
    "            itos,\n",
    "            max_total_tokens=max_total_tokens,\n",
    "            temperature=0.5,\n",
    "            top_k=None\n",
    "        )\n",
    "        end_t = time.time()\n",
    "        gen_t = end_t - start_t\n",
    "        times.append(gen_t)\n",
    "        results.append(generated_circuit_tokens)\n",
    "        print(f\"Generation time: {gen_t} secs, generated QAOA Circuit length: {len(generated_circuit_tokens)}\")\n",
    "\n",
    "    return results, times\n",
    "\n",
    "\n",
    "def char_accuracy(pred, target):\n",
    "    min_len = min(len(pred), len(target))\n",
    "    correct = sum(p == t for p, t in zip(pred[:min_len], target[:min_len]))\n",
    "    return correct / min_len\n",
    "\n",
    "\n",
    "def token_accuracy(pred_ids, target_ids):\n",
    "    min_len = min(len(pred_ids), len(target_ids))\n",
    "    correct = sum(p == t for p, t in zip(pred_ids[:min_len], target_ids[:min_len]))\n",
    "    return correct / min_len\n",
    "\n",
    "\n",
    "def levenshtein_distance(pred, target):\n",
    "    distance = Levenshtein.distance(pred, target)\n",
    "    norm_distance = distance / max(len(pred), len(target))\n",
    "    return distance, norm_distance\n",
    "\n",
    "\n",
    "def bleu_score(references, candidate):\n",
    "    bleu = sentence_bleu(references, candidate)\n",
    "    return bleu\n",
    "\n",
    "\n",
    "def save_seq_tokens_to_file(tokens: List[str], filename: str) -> None:\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(tokens, file)\n",
    "\n",
    "\n",
    "def load_seq_tokens_from_file(filename: str) -> List[str]:\n",
    "    with open(filename, \"rb\") as file:\n",
    "        loaded_list = pickle.load(file)\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load graphs and circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 88\n",
      "drwxr-xr-x 7 jovyan users  4096 Jul 11 02:57 .\n",
      "drwxr-xr-x 5 jovyan users  4096 May 29 07:10 ..\n",
      "drwxr-xr-x 2 jovyan users  4096 Jun 28 01:02 .ipynb_checkpoints\n",
      "drwxr-xr-x 3 jovyan users  4096 Jul 10 17:05 11_eleventh_batch_training\n",
      "drwxr-xr-x 2 jovyan users  4096 Jul 10 17:05 13_thirtheenth_batch_training\n",
      "-rw-r--r-- 1 jovyan users 39837 Jul 11 02:57 graphs_07_10_2025_10_38pm.pkl\n",
      "drwxr-xr-x 3 jovyan users  4096 Jul  8 01:10 graphs_diff_sizes\n",
      "-rw-r--r-- 1 jovyan users 19891 Jun 29 18:37 random_graphs_for_testing.pkl\n",
      "drwxr-xr-x 3 jovyan users  4096 Jul  8 00:55 third_batch\n"
     ]
    }
   ],
   "source": [
    "!ls -la ../data/graph_and_circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirname = \"../data/graph_and_circuits/third_batch\"\n",
    "dirname = \"../data/graph_and_circuits/11_eleventh_batch_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/graph_and_circuits/11_eleventh_batch_training/graphs11_batch1.pkl\n",
      "../data/graph_and_circuits/11_eleventh_batch_training/circuits11_batch1.qpy\n"
     ]
    }
   ],
   "source": [
    "dirobj = pathlib.Path(dirname)\n",
    "\n",
    "graphs_batch_files = sorted(list(dirobj.rglob(f\"*pkl\")))\n",
    "circuits_batch_files = sorted(list(dirobj.rglob(f\"*qpy\")))\n",
    "\n",
    "graphs_batch_file = graphs_batch_files[0]\n",
    "circuits_batch_file = circuits_batch_files[0]\n",
    "\n",
    "print(graphs_batch_file)\n",
    "print(circuits_batch_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 250, networkx.classes.graph.Graph)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs_batch = load_graphs(graphs_batch_file)\n",
    "\n",
    "type(graphs_batch), len(graphs_batch), type(graphs_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 250, qiskit.circuit.quantumcircuit.QuantumCircuit)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuits_batch = load_circuits(circuits_batch_file)\n",
    "\n",
    "type(circuits_batch), len(circuits_batch), type(circuits_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('rzz', 525), ('rxx', 525), ('ryy', 525), ('rz', 75), ('measure', 15), ('initialize', 1), ('barrier', 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1667"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gate_counts = circuits_batch[0].count_ops()\n",
    "print(gate_counts)\n",
    "sum(gate_counts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load random 15-nodes graphs for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 88\n",
      "drwxr-xr-x 7 jovyan users  4096 Jul 11 02:57 .\n",
      "drwxr-xr-x 5 jovyan users  4096 May 29 07:10 ..\n",
      "drwxr-xr-x 2 jovyan users  4096 Jun 28 01:02 .ipynb_checkpoints\n",
      "drwxr-xr-x 3 jovyan users  4096 Jul 10 17:05 11_eleventh_batch_training\n",
      "drwxr-xr-x 2 jovyan users  4096 Jul 10 17:05 13_thirtheenth_batch_training\n",
      "-rw-r--r-- 1 jovyan users 39837 Jul 11 02:57 graphs_07_10_2025_10_38pm.pkl\n",
      "drwxr-xr-x 3 jovyan users  4096 Jul  8 01:10 graphs_diff_sizes\n",
      "-rw-r--r-- 1 jovyan users 19891 Jun 29 18:37 random_graphs_for_testing.pkl\n",
      "drwxr-xr-x 3 jovyan users  4096 Jul  8 00:55 third_batch\n"
     ]
    }
   ],
   "source": [
    "!ls -la \"../data/graph_and_circuits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 <class 'list'> <class 'networkx.classes.graph.Graph'>\n"
     ]
    }
   ],
   "source": [
    "graphs_batch_file = \"../data/graph_and_circuits/random_graphs_for_testing.pkl\"\n",
    "graphs_batch = load_graphs(graphs_batch_file)\n",
    "print(len(graphs_batch), type(graphs_batch), type(graphs_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for graph in graphs_batch:\n",
    "    print(len(graph.nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load random 15-nodes graphs for testing (10 July 10:58pm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 88\n",
      "drwxr-xr-x 7 jovyan users  4096 Jul 11 02:57 .\n",
      "drwxr-xr-x 5 jovyan users  4096 May 29 07:10 ..\n",
      "drwxr-xr-x 2 jovyan users  4096 Jun 28 01:02 .ipynb_checkpoints\n",
      "drwxr-xr-x 3 jovyan users  4096 Jul 10 17:05 11_eleventh_batch_training\n",
      "drwxr-xr-x 2 jovyan users  4096 Jul 10 17:05 13_thirtheenth_batch_training\n",
      "-rw-r--r-- 1 jovyan users 39837 Jul 11 02:57 graphs_07_10_2025_10_38pm.pkl\n",
      "drwxr-xr-x 3 jovyan users  4096 Jul  8 01:10 graphs_diff_sizes\n",
      "-rw-r--r-- 1 jovyan users 19891 Jun 29 18:37 random_graphs_for_testing.pkl\n",
      "drwxr-xr-x 3 jovyan users  4096 Jul  8 00:55 third_batch\n"
     ]
    }
   ],
   "source": [
    "!ls -la \"../data/graph_and_circuits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 <class 'list'> <class 'networkx.classes.graph.Graph'>\n"
     ]
    }
   ],
   "source": [
    "graphs_batch_file = \"../data/graph_and_circuits/graphs_07_10_2025_10_38pm.pkl\"\n",
    "graphs_batch = load_graphs(graphs_batch_file)\n",
    "print(len(graphs_batch), type(graphs_batch), type(graphs_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for graph in graphs_batch:\n",
    "    print(len(graph.nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load graphs to generation time estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 864\n",
      "drwxr-xr-x 2 jovyan users  4096 Jun 28 01:03 .\n",
      "drwxr-xr-x 5 jovyan users  4096 Jun 29 18:37 ..\n",
      "-rw-r--r-- 1 jovyan users  9181 Jun 28 01:03 random_graph_5_10.pkl\n",
      "-rw-r--r-- 1 jovyan users 10991 Jun 28 01:03 random_graph_5_11.pkl\n",
      "-rw-r--r-- 1 jovyan users 12966 Jun 28 01:03 random_graph_5_12.pkl\n",
      "-rw-r--r-- 1 jovyan users 15109 Jun 28 01:03 random_graph_5_13.pkl\n",
      "-rw-r--r-- 1 jovyan users 17417 Jun 28 01:03 random_graph_5_14.pkl\n",
      "-rw-r--r-- 1 jovyan users 19890 Jun 28 01:03 random_graph_5_15.pkl\n",
      "-rw-r--r-- 1 jovyan users 22528 Jun 28 01:03 random_graph_5_16.pkl\n",
      "-rw-r--r-- 1 jovyan users 25331 Jun 28 01:03 random_graph_5_17.pkl\n",
      "-rw-r--r-- 1 jovyan users 28299 Jun 28 01:03 random_graph_5_18.pkl\n",
      "-rw-r--r-- 1 jovyan users 31432 Jun 28 01:03 random_graph_5_19.pkl\n",
      "-rw-r--r-- 1 jovyan users 34730 Jun 28 01:03 random_graph_5_20.pkl\n",
      "-rw-r--r-- 1 jovyan users 38190 Jun 28 01:03 random_graph_5_21.pkl\n",
      "-rw-r--r-- 1 jovyan users 41818 Jun 28 01:03 random_graph_5_22.pkl\n",
      "-rw-r--r-- 1 jovyan users 45611 Jun 28 01:03 random_graph_5_23.pkl\n",
      "-rw-r--r-- 1 jovyan users 49569 Jun 28 01:03 random_graph_5_24.pkl\n",
      "-rw-r--r-- 1 jovyan users 53692 Jun 28 01:03 random_graph_5_25.pkl\n",
      "-rw-r--r-- 1 jovyan users 57980 Jun 28 01:03 random_graph_5_26.pkl\n",
      "-rw-r--r-- 1 jovyan users 62433 Jun 28 01:03 random_graph_5_27.pkl\n",
      "-rw-r--r-- 1 jovyan users 67060 Jun 28 01:03 random_graph_5_28.pkl\n",
      "-rw-r--r-- 1 jovyan users 71843 Jun 28 01:03 random_graph_5_29.pkl\n",
      "-rw-r--r-- 1 jovyan users 76791 Jun 28 01:03 random_graph_5_30.pkl\n",
      "-rw-r--r-- 1 jovyan users  2651 Jun 28 01:03 random_graph_5_5.pkl\n",
      "-rw-r--r-- 1 jovyan users  3624 Jun 28 01:03 random_graph_5_6.pkl\n",
      "-rw-r--r-- 1 jovyan users  4756 Jun 28 01:03 random_graph_5_7.pkl\n",
      "-rw-r--r-- 1 jovyan users  6077 Jun 28 01:03 random_graph_5_8.pkl\n",
      "-rw-r--r-- 1 jovyan users  7563 Jun 28 01:03 random_graph_5_9.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls -la \"../data/graph_and_circuits/graphs_diff_sizes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/graph_and_circuits/graphs_diff_sizes/random_graph_5_5.pkl',\n",
       " '../data/graph_and_circuits/graphs_diff_sizes/random_graph_5_10.pkl',\n",
       " '../data/graph_and_circuits/graphs_diff_sizes/random_graph_5_15.pkl',\n",
       " '../data/graph_and_circuits/graphs_diff_sizes/random_graph_5_20.pkl',\n",
       " '../data/graph_and_circuits/graphs_diff_sizes/random_graph_5_25.pkl',\n",
       " '../data/graph_and_circuits/graphs_diff_sizes/random_graph_5_30.pkl']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(6, 5, list, networkx.classes.graph.Graph, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs_sizes = [5, 10, 15, 20, 25, 30]\n",
    "graphs_batch_files = [f\"../data/graph_and_circuits/graphs_diff_sizes/random_graph_5_{n}.pkl\" for n in graphs_sizes]\n",
    "display(graphs_batch_files)\n",
    "\n",
    "graphs_batches = [load_graphs(graphs_batch_file) for graphs_batch_file in graphs_batch_files]\n",
    "graphs_batch = graphs_batches[0]\n",
    "len(graphs_batches), len(graphs_batch), type(graphs_batch), type(graphs_batch[0]), len(graphs_batch[0].nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from a trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1668588\n",
      "drwxr-xr-x  3 jovyan users      4096 Jul 11 14:20 .\n",
      "drwxr-xr-x 10 jovyan users      4096 Jul 11 12:25 ..\n",
      "drwxr-xr-x  2 jovyan users      4096 Jul  8 21:46 .ipynb_checkpoints\n",
      "-rw-r--r--  1 jovyan users 538138348 Jul 11 14:25 ckpt.pt\n",
      "-rw-r--r--  1 jovyan users 646641388 Jul 10 16:19 ckpt_50m_20k_with_mu_and_format_v2.pt\n",
      "-rw-r--r--  1 jovyan users 523097836 Jul  9 19:46 ckpt_50m_20k_without_mu.pt\n",
      "-rw-r--r--  1 jovyan users    721254 Jul 11 14:20 meta_20k_old_format_and_without_mu.pkl\n"
     ]
    }
   ],
   "source": [
    "# !ls -la ../../nanoGPT_qaoa/out-graph-qaoa-100m/\n",
    "!ls -la ../../nanoGPT_qaoa/out-graph-qaoa-50m/\n",
    "# !ls -la ../../nanoGPT_qaoa/out-graph-qaoa-20m/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_from = \"resume\"\n",
    "# out_dir = \"../checkpoints\"\n",
    "# out_dir = \"../../nanoGPT_qaoa/out-graph-qaoa-100m/\"\n",
    "out_dir = \"../../nanoGPT_qaoa/out-graph-qaoa-50m/\"\n",
    "# out_dir = \"../../nanoGPT_qaoa/out-graph-qaoa-20m/\"\n",
    "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "seed = 1337\n",
    "\n",
    "# mps\n",
    "# device = \"mps\" # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "# # dtype = \"bfloat16\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"float16\" # 'float32' or 'bfloat16' or 'float16'\n",
    "# dtype = \"float32\" # for mac only\n",
    "# compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "# cpu\n",
    "# device = \"cpu\"    # run everything on CPU\n",
    "# dtype = \"float32\" # recommended for CPU (no bfloat16 or float16 support)\n",
    "# compile = False   # don't compile on CPU (PyTorch 2.0 compile gives little to no speedup or might even slow things down)\n",
    "\n",
    "# cuda\n",
    "device = \"cuda\" # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = \"float32\" # for mac only\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare torch and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = \"cuda\" if \"cuda\" in device else \"cpu\" # for later use in torch.autocast\n",
    "ptdtype = {\"float32\": torch.float32, \"bfloat16\": torch.bfloat16, \"float16\": torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == \"cpu\" else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load meta info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 769754 Jul 10 22:05 ../../nanoGPT_qaoa/data/graph_qaoa/meta.pkl\n",
      "-rw-r--r-- 1 jovyan users 653602 Jul 11 15:02 ../../nanoGPT_qaoa/data/graph_qaoa/meta_100m.pkl\n",
      "-rw-r--r-- 1 jovyan users 721254 Jul 11 14:25 ../../nanoGPT_qaoa/data/graph_qaoa/meta_20k_old_format_and_without_mu.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls -la ../../nanoGPT_qaoa/data/graph_qaoa/meta*.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta from ../../nanoGPT_qaoa/data/graph_qaoa/meta_20k_old_format_and_without_mu.pkl...\n",
      "vocab_size=34937\n"
     ]
    }
   ],
   "source": [
    "# meta_path = \"../data/train/meta.pkl\"\n",
    "# meta_path = \"../../nanoGPT_qaoa/data/graph_qaoa/meta_100m.pkl\" # old 100M model\n",
    "# meta_path = \"../../nanoGPT_qaoa/data/graph_qaoa/meta.pkl\" # new model\n",
    "meta_path = \"../../nanoGPT_qaoa/data/graph_qaoa/meta_20k_old_format_and_without_mu.pkl\" # old model trained on 20k without nodes weights\n",
    "\n",
    "print(f\"Loading meta from {meta_path}...\")\n",
    "with open(meta_path, \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "vocab_size = meta[\"vocab_size\"]\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "pad_token = meta.get(\"pad_token\", \"<|pad|>\")\n",
    "pad_token_id = stoi.get(pad_token, 0)\n",
    "encode = lambda s: [stoi.get(c, pad_token_id) for c in s]\n",
    "decode = lambda l: \"\".join([itos.get(i, \"\") for i in l if i in itos])\n",
    "\n",
    "print(f\"{vocab_size=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 43.06M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ckpt_path = os.path.join(out_dir, \"ckpt.pt\") # new model\n",
    "ckpt_path = os.path.join(out_dir, \"ckpt_50m_20k_without_mu.pt\") # old model trained on 20k without nodes weights\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint[\"model_args\"])\n",
    "gptconf.stop_token_id = meta[\"stop_token_id\"]\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint[\"model\"]\n",
    "unwanted_prefix = \"_orig_mod.\"\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_qaoa_cached.GPT"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model was compiled successfully\n"
     ]
    }
   ],
   "source": [
    "if device == \"cuda\":\n",
    "    # for cuda\n",
    "    model = model.to(torch.device(\"cuda\"))\n",
    "    model = model.to(dtype=torch.bfloat16) # optional, for speed\n",
    "    if compile:\n",
    "        model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "        model.eval()\n",
    "        print(\"model was compiled successfully\")\n",
    "else:\n",
    "    # for cpu and mps\n",
    "    print(\"model on CPU\")\n",
    "    model.eval()\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch._dynamo.eval_frame.OptimizedModule"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use any graph from loaded batch as a test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graphs_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'return_': np.float64(0.9735609521124655)}\n",
      "{'return_': np.float64(6.500412732811528)}\n",
      "{'return_': np.float64(1.448465898264445)}\n",
      "{'return_': np.float64(0.7047703914889547)}\n",
      "{'return_': np.float64(8.789848381367007)}\n",
      "{'return_': np.float64(1.7476096289867482)}\n",
      "{'return_': np.float64(5.854471537792451)}\n",
      "{'return_': np.float64(1.3033649118656943)}\n",
      "{'return_': np.float64(9.727996690806933)}\n",
      "{'return_': np.float64(6.903007524592617)}\n",
      "{'return_': np.float64(6.61860447340752)}\n",
      "{'return_': np.float64(6.746837841142661)}\n",
      "{'return_': np.float64(2.3683161548347753)}\n",
      "{'return_': np.float64(0.8159069379167927)}\n",
      "{'return_': np.float64(9.831672007484308)}\n"
     ]
    }
   ],
   "source": [
    "tmp_g = graphs_batch[0]\n",
    "for u in tmp_g.nodes:\n",
    "    print(tmp_g.nodes[u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n",
      "['<bos>', '(0,1)', '0.20', '(0,2)', '0.59', '(0,3)', '0.68', '(0,4)', '0.74', '(0,5)', '1.00', '(0,6)', '0.38', '(0,7)', '0.89', '(0,8)', '0.45', '(0,9)', '0.01', '(0,10)', '0.88', '(0,11)', '0.39', '(0,12)', '0.03', '(0,13)', '0.44', '(0,14)', '0.34', '(1,2)', '0.74', '(1,3)', '0.74', '(1,4)', '0.04', '(1,5)', '0.81', '(1,6)', '0.51', '(1,7)', '0.20', '(1,8)', '0.14', '(1,9)', '0.65', '(1,10)', '0.40', '(1,11)', '0.58', '(1,12)', '0.21', '(1,13)', '0.67', '(1,14)', '0.43', '(2,3)', '0.40', '(2,4)', '0.92', '(2,5)', '0.40', '(2,6)', '0.11', '(2,7)', '0.90', '(2,8)', '0.03', '(2,9)', '0.06', '(2,10)', '0.63', '(2,11)', '0.77', '(2,12)', '0.84', '(2,13)', '0.53', '(2,14)', '0.09', '(3,4)', '1.00', '(3,5)', '0.68', '(3,6)', '0.54', '(3,7)', '0.30', '(3,8)', '0.88', '(3,9)', '0.86', '(3,10)', '0.32', '(3,11)', '0.81', '(3,12)', '0.94', '(3,13)', '0.56', '(3,14)', '0.01', '(4,5)', '0.70', '(4,6)', '0.49', '(4,7)', '0.53', '(4,8)', '0.34', '(4,9)', '0.63', '(4,10)', '0.92', '(4,11)', '0.56', '(4,12)', '0.11', '(4,13)', '0.99', '(4,14)', '0.63', '(5,6)', '0.06', '(5,7)', '0.13', '(5,8)', '0.45', '(5,9)', '0.64', '(5,10)', '0.39', '(5,11)', '0.72', '(5,12)', '0.81', '(5,13)', '0.29', '(5,14)', '0.53', '(6,7)', '0.90', '(6,8)', '0.74', '(6,9)', '0.26', '(6,10)', '0.99', '(6,11)', '0.16', '(6,12)', '0.34', '(6,13)', '0.96', '(6,14)', '0.83', '(7,8)', '0.49', '(7,9)', '0.81', '(7,10)', '0.09', '(7,11)', '0.18', '(7,12)', '0.16', '(7,13)', '0.90', '(7,14)', '0.70', '(8,9)', '0.77', '(8,10)', '0.34', '(8,11)', '0.23', '(8,12)', '0.63', '(8,13)', '0.73', '(8,14)', '0.48', '(9,10)', '1.00', '(9,11)', '0.32', '(9,12)', '0.15', '(9,13)', '0.37', '(9,14)', '0.60', '(10,11)', '0.01', '(10,12)', '0.22', '(10,13)', '0.03', '(10,14)', '0.34', '(11,12)', '0.30', '(11,13)', '0.44', '(11,14)', '0.29', '(12,13)', '0.55', '(12,14)', '0.36', '(13,14)', '0.23', '<end_of_graph>']\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "graph = graphs_batch[sample_idx]\n",
    "# graph_tokens = graph_to_tokens_v1(graph) # for the new model\n",
    "graph_tokens = graph_to_tokens_old_format(graph) # for the old model\n",
    "\n",
    "# circuit = circuits_batch[sample_idx]        # comment if use random graphs\n",
    "# circuit_tokens = circuit_to_tokens_v1(circuit) # comment if use random graphs\n",
    "\n",
    "print(len(graph_tokens))\n",
    "print(graph_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_to_str_v1(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55810,\n",
       " '<new_layer_p>, rz, [0], -8.89 <new_layer_p>, rz, [1], -8.82 <new_layer_p>, rzz, [0, 1], 1.27 <new_layer_p>, rz, [2], -8.88 <new_layer_p>, rzz, [0, 2], 1.27 <new_layer_p>, rzz, [1, 2], 1.27 <new_layer_p>, rz, [3], -8.82 <new_layer_p>, rzz, [0, 3], 1.27 <new_layer_p>, rzz, [1, 3], 1.27 <new_layer_p>, rzz, [2, 3], 1.27 <new_layer_p>, rz, [4], -8.85 <new_layer_p>, rzz, [0, 4], 1.27 <new_layer_p>, rzz, [1, 4], 1.27 <new_layer_p>, rzz, [2, 4], 1.27 <new_layer_p>, rzz, [3, 4], 1.27 <new_layer_p>, rz, [5], -8.84 <new_layer_p>, rzz, [0, 5], 1.27 <new_layer_p>, rzz, [1, 5], 1.27 <new_layer_p>, rzz, [2, 5], 1.27 <new_layer_p>, rzz, [3, 5], 1.27 <new_layer_p>, rzz, [4, 5], 1.27 <new_layer_p>, rz, [6], -8.87 <new_layer_p>, rzz, [0, 6], 1.27 <new_layer_p>, rzz, [1, 6], 1.27 <new_layer_p>, rzz, [2, 6], 1.27 <new_layer_p>, rzz, [3, 6], 1.27 <new_layer_p>, rzz, [4, 6], 1.27 <new_layer_p>, rzz, [5, 6], 1.27 <new_layer_p>, rz, [7], -8.86 <new_layer_p>, rzz, [0, 7], 1.27 <new_layer_p>, rzz, [1, 7], 1.27 <')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(circuit_to_str_v1(circuit)), circuit_to_str_v1(circuit)[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run generation (batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 1386, 10: 3465, 15: 6930, 20: 12127, 25: 18018, 30: 24255}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For 15-nodes graph the number of tokens in qiskit circuit - 6600\n",
    "# Therefore, for 5 nodes shoud be approx. (# of gates for 5 nodes / # of gates for 15 nodes) * 6600 and add extra 5%\n",
    "qokit_number_of_gates = {\n",
    "    5: 200,   \n",
    "    10: 500,\n",
    "    15: 1000,\n",
    "    20: 1750,\n",
    "    25: 2600,\n",
    "    30: 3500,\n",
    "}\n",
    "\n",
    "max_total_tokens_dict = {}\n",
    "for k, v in qokit_number_of_gates.items():\n",
    "    max_total_tokens_dict[k] = int((v / qokit_number_of_gates[15]) * 6600 * 1.05)\n",
    "\n",
    "max_total_tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphs with 5 nodes are processing\n",
      "max_total_tokens=1386\n",
      "Warm-up model\n",
      "Elapsed time for warm-up: 0.004258155822753906 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|        | 1/5 [00:05<00:22,  5.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 5.71644926071167 secs, generated QAOA Circuit length: 1364\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|      | 2/5 [00:11<00:17,  5.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 5.643535852432251 secs, generated QAOA Circuit length: 1364\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|    | 3/5 [00:17<00:11,  5.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 5.670839786529541 secs, generated QAOA Circuit length: 1364\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|  | 4/5 [00:22<00:05,  5.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 5.588042259216309 secs, generated QAOA Circuit length: 1364\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 5/5 [00:28<00:00,  5.68s/it]\u001b[A\n",
      " 17%|        | 1/6 [00:28<02:22, 28.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 5.781782150268555 secs, generated QAOA Circuit length: 1364\n",
      "graphs with 10 nodes are processing\n",
      "max_total_tokens=3465\n",
      "Warm-up model\n",
      "Elapsed time for warm-up: 0.004490375518798828 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|        | 1/5 [00:14<00:56, 14.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 14.121472120285034 secs, generated QAOA Circuit length: 3373\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|      | 2/5 [00:28<00:41, 14.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 13.908310413360596 secs, generated QAOA Circuit length: 3373\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|    | 3/5 [00:42<00:28, 14.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 14.043062925338745 secs, generated QAOA Circuit length: 3373\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|  | 4/5 [00:56<00:13, 13.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 13.946916103363037 secs, generated QAOA Circuit length: 3373\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 5/5 [01:10<00:00, 14.01s/it]\u001b[A\n",
      " 33%|      | 2/6 [01:38<03:31, 52.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 14.03970742225647 secs, generated QAOA Circuit length: 3373\n",
      "graphs with 15 nodes are processing\n",
      "max_total_tokens=6930\n",
      "Warm-up model\n",
      "Elapsed time for warm-up: 0.0044710636138916016 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|        | 1/5 [00:27<01:51, 27.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 27.761804342269897 secs, generated QAOA Circuit length: 6718\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|      | 2/5 [00:55<01:23, 27.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 27.889658212661743 secs, generated QAOA Circuit length: 6718\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|    | 3/5 [01:23<00:55, 27.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 28.038382053375244 secs, generated QAOA Circuit length: 6718\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|  | 4/5 [01:51<00:27, 27.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 27.761962413787842 secs, generated QAOA Circuit length: 6718\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 5/5 [02:19<00:00, 27.86s/it]\u001b[A\n",
      " 50%|     | 3/6 [03:57<04:37, 92.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 27.836496829986572 secs, generated QAOA Circuit length: 6718\n",
      "graphs with 20 nodes are processing\n",
      "max_total_tokens=12127\n",
      "Warm-up model\n",
      "Elapsed time for warm-up: 0.004651784896850586 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|        | 1/5 [00:49<03:17, 49.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 49.301159143447876 secs, generated QAOA Circuit length: 11745\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|      | 2/5 [01:38<02:27, 49.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 49.181010007858276 secs, generated QAOA Circuit length: 11745\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|    | 3/5 [02:27<01:38, 49.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 49.12739109992981 secs, generated QAOA Circuit length: 11745\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|  | 4/5 [03:16<00:49, 49.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 48.73175859451294 secs, generated QAOA Circuit length: 11745\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 5/5 [04:05<00:00, 49.04s/it]\u001b[A\n",
      " 67%|   | 4/6 [08:02<05:05, 152.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 48.83674144744873 secs, generated QAOA Circuit length: 11745\n",
      "graphs with 25 nodes are processing\n",
      "max_total_tokens=18018\n",
      "Warm-up model\n",
      "Elapsed time for warm-up: 0.004756927490234375 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|        | 1/5 [01:12<04:50, 72.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 72.73330521583557 secs, generated QAOA Circuit length: 17416\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|      | 2/5 [02:25<03:38, 72.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 72.85814237594604 secs, generated QAOA Circuit length: 17416\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|    | 3/5 [03:38<02:25, 72.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 72.95708799362183 secs, generated QAOA Circuit length: 17416\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|  | 4/5 [04:51<01:12, 72.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 73.15130972862244 secs, generated QAOA Circuit length: 17416\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 5/5 [06:04<00:00, 72.89s/it]\u001b[A\n",
      " 83%| | 5/6 [14:07<03:49, 229.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 72.75522494316101 secs, generated QAOA Circuit length: 17416\n",
      "graphs with 30 nodes are processing\n",
      "max_total_tokens=24255\n",
      "Warm-up model\n",
      "Elapsed time for warm-up: 0.005433082580566406 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|        | 1/5 [01:42<06:50, 102.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 102.51838946342468 secs, generated QAOA Circuit length: 23383\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|      | 2/5 [03:25<05:07, 102.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 102.49922108650208 secs, generated QAOA Circuit length: 23383\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|    | 3/5 [05:07<03:25, 102.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 102.76785612106323 secs, generated QAOA Circuit length: 23383\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|  | 4/5 [06:50<01:42, 102.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 103.07900595664978 secs, generated QAOA Circuit length: 23383\n",
      "device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|| 5/5 [08:34<00:00, 102.84s/it]\u001b[A\n",
      "100%|| 6/6 [22:41<00:00, 226.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 103.30233693122864 secs, generated QAOA Circuit length: 23383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for graphs_batch in tqdm.tqdm(graphs_batches):\n",
    "    n = len(graphs_batch[0].nodes)\n",
    "    print(f\"graphs with {n} nodes are processing\")\n",
    "\n",
    "    max_total_tokens = max_total_tokens_dict[n]    \n",
    "    print(f\"{max_total_tokens=}\")\n",
    "\n",
    "    generated_circuits, times = generate_batch(graphs_batch, model, gptconf, stoi, itos, max_total_tokens, device)\n",
    "    results[n] = {\n",
    "        \"circuits\": generated_circuits,\n",
    "        \"times\": times,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.71644926071167, 5.643535852432251, 5.670839786529541, 5.588042259216309, 5.781782150268555]\n",
      "[14.121472120285034, 13.908310413360596, 14.043062925338745, 13.946916103363037, 14.03970742225647]\n",
      "[27.761804342269897, 27.889658212661743, 28.038382053375244, 27.761962413787842, 27.836496829986572]\n",
      "[49.301159143447876, 49.181010007858276, 49.12739109992981, 48.73175859451294, 48.83674144744873]\n",
      "[72.73330521583557, 72.85814237594604, 72.95708799362183, 73.15130972862244, 72.75522494316101]\n",
      "[102.51838946342468, 102.49922108650208, 102.76785612106323, 103.07900595664978, 103.30233693122864]\n"
     ]
    }
   ],
   "source": [
    "# save times\n",
    "graphs_sizes = [5, 10, 15, 20, 25, 30]\n",
    "for n in graphs_sizes:\n",
    "    print(results[n][\"times\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Node Size</th>\n",
       "      <th>Mean Time</th>\n",
       "      <th>Standard Deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5.680130</td>\n",
       "      <td>0.065663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>14.011894</td>\n",
       "      <td>0.075768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>27.857661</td>\n",
       "      <td>0.102449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>49.035612</td>\n",
       "      <td>0.215386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>72.891014</td>\n",
       "      <td>0.152651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>102.833362</td>\n",
       "      <td>0.314778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Node Size   Mean Time  Standard Deviation\n",
       "0          5    5.680130            0.065663\n",
       "1         10   14.011894            0.075768\n",
       "2         15   27.857661            0.102449\n",
       "3         20   49.035612            0.215386\n",
       "4         25   72.891014            0.152651\n",
       "5         30  102.833362            0.314778"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Node Size\": [5, 10, 15, 20, 25, 30],\n",
    "    \"Mean Time\": [np.mean(results[n][\"times\"]) for n in graphs_sizes],\n",
    "    \"Standard Deviation\": [np.std(results[n][\"times\"]) for n in graphs_sizes]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"qpt_generation_time.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save generated cirtuits\n",
    "\n",
    "# def save_seq_tokens_to_file(tokens: List[str], filename: str) -> None:\n",
    "#     with open(filename, \"wb\") as file:\n",
    "#         pickle.dump(tokens, file)\n",
    "\n",
    "# # generated sequence\n",
    "# save_seq_tokens_to_file(generated_circuit_tokens, filename_gen)\n",
    "# generated_circuit_tokens_loaded = load_seq_tokens_from_file(filename_gen)\n",
    "# any([e1 == e2 for e1, e2 in zip(generated_circuit_tokens, generated_circuit_tokens_loaded)])\n",
    "\n",
    "graphs_sizes = [5, 10, 15, 20, 25, 30]\n",
    "for n in graphs_sizes:\n",
    "    generated_circuits = results[n][\"circuits\"]\n",
    "    filename_gen = f\"generated_circuit_tokens_nodes_{n:d}_amount_{len(generated_circuits):d}.pkl\"\n",
    "    save_seq_tokens_to_file(generated_circuits, filename_gen)\n",
    "    generated_circuit_loaded = load_seq_tokens_from_file(filename_gen)\n",
    "    for c_gen, c_gen_loaded in zip(generated_circuits, generated_circuit_loaded):\n",
    "        if not any([e1 == e2 for e1, e2 in zip(c_gen, c_gen_loaded)]):\n",
    "            print(f\"Something was wrong during saving\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " 6718,\n",
       " ['<new_layer_p>',\n",
       "  'rz',\n",
       "  '[0]',\n",
       "  '-9.82',\n",
       "  '<new_layer_p>',\n",
       "  'rz',\n",
       "  '[1]',\n",
       "  '-9.84',\n",
       "  '<new_layer_p>',\n",
       "  'rz'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"generated_circuit_tokens_nodes_15_amount_5.pkl\"\n",
    "tmp = load_seq_tokens_from_file(filename)\n",
    "len(tmp), len(tmp[1]), tmp[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run generation (single input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "device: cuda:0\n",
      "Generation time: 40.35027718544006 secs\n",
      "Generated QAOA Circuit (length: 6788):\n",
      "<new_layer_p> rz [13] -9.29 <new_layer_p> rz [14] 207.99 <new_layer_p> rzz [11, 12] 0.25 (11,14) 0.70 (11,14) 0.61 (4,7) 0.82 <end_of_graph> <new_layer_p> rzz [4, 5] 0.33 (4,7) 0.70 <end_of_graph> <new_layer_p> rzz [12, 13] 1.77 <new_layer_p> rzz [1, 3] 13.58 <new_layer_p> rzz [2, 6] 173.56 <new_layer_p> rxx [5, 12] 0.92 <new_layer_p> rzz [0, 2] 15.05 <new_layer_p> rzz [0, 13] 209.79 <new_layer_p> rzz [0, 11] 0.14 (12,14) 0.80 <end_of_graph> <new_layer_p> rzz [9, 10] 191.81 <new_layer_p> rzz [4, 10] 0.90 (4,7) 0.14 (11,13) 0.32 (7,10) 0.32 (13,14) 0.65 (4,7) 0.47 (4,7) 0.34 (4,5) 0.08 (4,7) 0.61 (4,7) 0.25 (11,14) 0.25 (1,13) 0.97 (13,14) 0.40 (10,14) 0.72 (13,14) 0.65 (10,14) 0.55 (13,14) 0.93 <new_layer_p> rzz [5, 14] rxx [2, 13] 0.82 <end_of_graph> <new_layer_p> ryy [4, 5] 0.22 (13,14) 0.65 (3,8) 0.69 <end_of_graph> <new_layer_p> rzz [5, 10] 0.65 (7,12) <new_layer_p> rzz [4, 8] 0.79 <end_of_graph> <new_layer_p> rzz [3, 5] 0.31 (4,7) 0.29 (10,14) 0.56 (4,7) 0.40 (10,14) rzz [2, 5] 0.40 (7,11) 0.55 (6,9) 0.13 (8,14) 0.08 (6,14) 0.54 <end_of_graph> <new_layer_p> rzz [3, 13] 216.20 <new_layer_p> rzz [10, 12] 0.83 (10,14) 0.51 (10,13) 0.40 (6,12) 0.72 <end_of_graph> <new_layer_p> rzz [3, 4] 220.44 <new_layer_p> rxx [6, 14] 0.81 (9,14) 0.56 (11,14) 0.59 <new_layer_p> rxx [2, 7] 193.39 <new_layer_p> rzz [5, 12] 1.57 <new_layer_p> rxx [0, 1] 1.05 <new_layer_p> rzz [1, 12] 1.57 <new_layer_p> rxx [2, 5] 216.36 <new_layer_p> rzz [3, 6] 1.32 <new_layer_p> rzz [7, 14] 207.72 <new_layer_p> rzz [3, 8] 1.26 <new_layer_p> rzz [3, 5] 0.21 (11,14) 0.25 (6,14) 0.26 (4,7) 0.65 (10,14) 0.56 (6,12) 0.65 (10,14) 0.57 (12,14) 0.89 <new_layer_p> ryy [0, 8] 1.36 <new_layer_p> rxx [0, 4] 158.90 <new_layer_p> rzz [4, 11] 1.49 <new_layer_p> rzz [10, 13] 1.28 <new_layer_p> rzz [1, 6] 173.56 <new_layer_p> rxx [4, 9] 1.20 <new_layer_p> rxx [8, 9] 18.25 <new_layer_p> rzz [8, 9] 198.11 <new_layer_p> rzz [7, 9] 1.29 <new_layer_p> rzz [1, 5] 1.30 <new_layer_p> rzz [3, 8] 0.56 (6,9) 0.76 (9,11) 0.01 <new_layer_p> rzz [7, 11] 200.71 <new_layer_p> rzz [2, 12] 1.33 <new_layer_p> rxx [2, 7] 1.36 <new_layer_p> rxx [3, 13] 1.18 <new_layer_p> rxx [6, 14] 1.39 <new_layer_p> rzz [0, 11] 217.92 <new_layer_p> rzz [13, 14] rzz [6, 7] 1.27 <new_layer_p> rxx [4, 10] 1.17 <new_layer_p> rzz [0, 8] 1.28 <new_layer_p> ryy [10, 14] 179.46 <new_layer_p> rz [5] -10.18 <new_layer_p> ryy [7, 11] 188.34 <new_layer_p> ryy [3, 13] 1.28 <new_layer_p> rzz [2, 4] 0.72 (10,14) 0.40 (10,14) 0.57 (10,14) rzz [3, 12] 1.31 <new_layer_p> rzz [8, 12] 201.38 <new_layer_p> rxx [1, 11] 1.49 <new_layer_p> ryy [10, 12] 219.20 <new_layer_p> rzz [1, 5] 1.16 <new_layer_p> ryy [6, 7] 1.27 <new_layer_p> rzz [4, 13] 1.27 <new_layer_p> rzz [1, 5] 211.83 <new_layer_p> rxx [2, 13] 184.05 <new_layer_p> rxx [8, 14] 188.07 <new_layer_p> rzz [6, 14] 0.21 (8,11) 0.36 (8,12) 0.98 (10,14) 0.57 (10,14) rzz [7, 11] 1.33 <new_layer_p> rxx [0, 11] 198.18 <new_layer_p> rzz [3, 14] 1.32 <new_layer_p> ryy [0, 4] 228.83 <new_layer_p> rxx [0, 9] 228.75 <new_layer_p> rzz [0, 8] 1.33 <new_layer_p> rxx [0, 1] 211.13 <new_layer_p> rxx [3, 8] 1.36 <new_layer_p> rzz [8, 13] 1.33 <new_layer_p> rzz [1, 6] 213.66 <new_layer_p> ryy [0, 3] 1.33 <new_layer_p> rxx [0, 1] 1.27 <new_layer_p> ryy [3, 10] 209.75 <new_layer_p> rzz [3, 8] 1.32 <new_layer_p> rzz [2, 13] 209.63 <new_layer_p> rxx [13, 14] rzz [2, 4] 1.17 <new_layer_p> rzz [0, 13] 1.18 <new_layer_p> rzz [7, 8] 1.39 <new_layer_p> ryy [7, 10] 201.30 <new_layer_p> rzz [3, 12] 1.25 <new_layer_p> rzz [6, 8] 174.47 <new_layer_p> rxx [4, 5] 210.71 <new_layer_p> ryy [3, 14] 1.17 <new_layer_p> rzz [2, 11] 1.36 <new_layer_p> rxx [5, 7] 1.39 <new_layer_p> rzz [10, 11] 0.90 (3,12) 0.65 (0,8) 0.08 (8,9) 0.08 (10,14) 0.35 (7,12) 0.05 <new_layer_p> rxx [2, 11] 1.19 <new_layer_p> rzz [5, 6] 1.36 <new_layer_p> rxx [4, 11] 211.11 <new_layer_p> rxx [0, 7] 212.38 <new_layer_p> rzz [3, 11] 227.41 <new_layer_p> rzz [6, 12] 1.98 <new_layer_p> ryy [4, 11] 210.77 <new_layer_p> rzz [8, 14] 1.26 <new_layer_p> ryy [0, 4] 1.32 <new_layer_p> rzz [7, 8] 214.58 <new_layer_p> rzz [0, 9] 216.45 <new_layer_p> rxx [9, 10] 1.36 <new_layer_p> rzz [6, 13] 1.58 <new_layer_p> rxx [5, 7] 198.26 <new_layer_p> rxx [0, 6] 1.26 <new_layer_p> ryy [3, 12] 1.51 <new_layer_p> rzz [4, 9] 1.28 <new_layer_p> rxx [1, 13] 1.33 <new_layer_p> rzz [9, 13] 13.56 <new_layer_p> rxx [6, 11] 219.11 <new_layer_p> rxx [9, 12] 1.38 <new_layer_p> rxx [0, 7] 1.28 <new_layer_p> rzz [0, 10] 1.35 <new_layer_p> rxx [0, 1] 1.15 <new_layer_p> rzz [3, 8] 1.36 <new_layer_p> rzz [4, 8] 1.36 <new_layer_p> rzz [6, 14] 1.45 <new_layer_p> rzz [9, 10] 1.33 <new_layer_p> rzz [0, 7] 208.13 <new_layer_p> rxx [10, 12] 1.39 <new_layer_p> rzz [2, 14] 1.28 <new_layer_p> rzz [12, 14] 15.05 <new_layer_p> rzz [10, 12] 1.40 <new_layer_p> rxx [4, 12] 1.28 <new_layer_p> rzz [3, 13] 1.25 <new_layer_p> rzz [3, 13] 1.28 <new_layer_p> rzz [6, 7] 200.16 <new_layer_p> rzz [3, 7] 1.32 <new_layer_p> rzz [9, 11] 1.39 <new_layer_p> rxx [2, 9] 228.57 <new_layer_p> rzz [2, 12] 1.28 <new_layer_p> rxx [7, 10] 1.37 <new_layer_p> rzz [0, 8] 180.53 <new_layer_p> rzz [4, 13] 1.16 <new_layer_p> rxx [6, 7] 0.86 <new_layer_p> rxx [5, 10] 1.39 <new_layer_p> rzz [3, 11] 1.37 <new_layer_p> rxx [7, 12] 1.18 <new_layer_p> rzz [8, 10] 1.28 <new_layer_p> rzz [4, 11] 1.28 <new_layer_p> rxx [6, 11] 214.39 <new_layer_p> rzz [0, 9] 1.45 <new_layer_p> rzz [5, 9] 1.27 <new_layer_p> ryy [5, 9] 1.35 <new_layer_p> rxx [5, 6] 1.34 <new_layer_p> rzz [3, 10] 209.75 <new_layer_p> rzz [1, 13] 1.33 <new_layer_p> rzz [1, 6] 1.18 <new_layer_p> ryy [5, 13] 1.49 <new_layer_p> rzz [12, 14] 3.14 <new_layer_p> rxx [1, 3] 1.17 <new_layer_p> rzz [0, 13] 198.40 <new_layer_p> rzz [1, 4] 230.30 <new_layer_p> rxx [1, 4] 1.28 <new_layer_p> rxx [1, 13] 1.55 <new_layer_p> rzz [8, 11] 1.11 <new_layer_p> ryy [3, 9] 1.14 <new_layer_p> ryy [6, 8] 1.28 <new_layer_p> rzz [2, 11] 1.28 <new_layer_p> rzz [11, 14] 1.53 <new_layer_p> rzz [4, 14] 1.36 <new_layer_p> rzz [4, 10] 1.57 <new_layer_p> rxx [1, 10] 1.16 <new_layer_p> ryy [5, 10] 1.39 <new_layer_p> rzz [1, 6] 1.55 <new_layer_p> rzz [3, 12] 1.41 <new_layer_p> rxx [2, 6] 214.59 <new_layer_p> rxx [1, 14] 1.36 <new_layer_p> rzz [4, 11] 1.24 <new_layer_p> ryy [9, 10] 1.17 <new_layer_p> rzz [3, 10] 1.33 <new_layer_p> rxx [1, 9] 1.26 <new_layer_p> ryy [7, 8] 214.30 <new_layer_p> rxx [3, 12] 1.28 <new_layer_p> rzz [1, 7] 1.18 <new_layer_p> rzz [8, 11] 1.35 <new_layer_p> rzz [8, 14] 1.40 <new_layer_p> rzz [2, 6] 209.20 <new_layer_p> rzz [4, 7] 220.66 <new_layer_p> rxx [0, 11] 1.33 <new_layer_p> ryy [2, 9] 1.39 <new_layer_p> rxx [8, 13] 1.46 <new_layer_p> ryy [12, 14] 180.05 <new_layer_p> rzz [1, 7] 1.27 <new_layer_p> rzz [2, 9] 1.33 <new_layer_p> rxx [3, 9] 1.27 <new_layer_p> rzz [2, 4] 1.16 <new_layer_p> rzz [4, 6] 192.35 <new_layer_p> rzz [1, 12] 1.35 <new_layer_p> rxx [6, 10] 214.39 <new_layer_p> rzz [9, 10] 1.32 <new_layer_p> rzz [6, 14] 1.33 <new_layer_p> rzz [3, 13] 1.36 <new_layer_p> rzz [1, 5] 1.33 <new_layer_p> rzz [6, 9] 1.17 <new_layer_p> rxx [9, 10] 1.32 <new_layer_p> ryy [5, 14] 212.11 <new_layer_p> rxx [3, 12] 1.37 <new_layer_p> rzz [6, 13] 1.58 <new_layer_p> rzz [5, 10] 1.33 <new_layer_p> ryy [3, 7] 1.33 <new_layer_p> rzz [7, 8] 1.39 <new_layer_p> rxx [6, 8] 1.37 <new_layer_p> rzz [9, 11] 1.33 <new_layer_p> rzz [12, 14] 1.36 <new_layer_p> rxx [0, 1] 1.27 <new_layer_p> rzz [8, 12] 1.07 <new_layer_p> rxx [8, 11] 1.35 <new_layer_p> rzz [4, 9] 1.37 <new_layer_p> rxx [8, 10] 1.11 <new_layer_p> rzz [9, 10] 191.46 <new_layer_p> rzz [5, 8] 1.21 <new_layer_p> ryy [3, 11] 1.28 <new_layer_p> rzz [12, 14] 1.33 <new_layer_p> ryy [1, 10] 1.29 <new_layer_p> ryy [4, 5] 1.36 <new_layer_p> ryy [6, 14] 1.36 <new_layer_p> rzz [3, 12] 1.41 <new_layer_p> rxx [4, 14] 1.36 <new_layer_p> rzz [4, 13] 1.17 <new_layer_p> rxx [0, 1] 1.17 <new_layer_p> ryy [1, 3] 1.27 <new_layer_p> ryy [1, 3] 1.43 <new_layer_p> rzz [4, 10] 1.36 <new_layer_p> rzz [8, 11] 1.39 <new_layer_p> rzz [13, 14] 1.33 <new_layer_p> rzz [0, 9] 1.36 <new_layer_p> ryy [5, 9] 1.33 <new_layer_p> rzz [0, 11] 1.39 <new_layer_p> rxx [4, 6] 1.39 <new_layer_p> rxx [6, 9] 1.45 <new_layer_p> rzz [7, 13] 198.51 <new_layer_p> rxx [0, 5] 1.29 <new_layer_p> rzz [2, 6] 217.42 <new_layer_p> ryy [2, 5] 216.36 <new_layer_p> rxx [8, 12] 1.45 <new_layer_p> rzz [3, 10] 1.32 <new_layer_p> ryy [4, 11] 1.57 <new_layer_p> ryy [2, 14] 1.33 <new_layer_p> rxx [1, 3] 1.33 <new_layer_p> ryy [2, 14] 1.34 <new_layer_p> rzz [12, 13] 1.45 <new_layer_p> rzz [10, 11] 1.33 <new_layer_p> rzz [0, 14] 1.36 <new_layer_p> ryy [3, 12] 1.31 <new_layer_p> rzz [3, 13] 1.57 <new_layer_p> rzz [9, 13] 1.24 <new_layer_p> rzz [9, 14] 1.41 <new_layer_p> rzz [0, 5] 1.38 <new_layer_p> ryy [3, 12] 1.35 <new_layer_p> rxx [1, 9] 1.28 <new_layer_p> rzz [3, 14] 1.17 <new_layer_p> rxx [3, 12] 1.21 <new_layer_p> rzz [0, 6] 1.36 <new_layer_p> rzz [1, 7] 1.33 <new_layer_p> rxx [8, 14] 1.15 <new_layer_p> rzz [2, 13] 1.27 <new_layer_p> rzz [8, 13] 1.28 <new_layer_p> rxx [3, 7] 1.35 <new_layer_p> rxx [4, 8] 1.45 <new_layer_p> rzz [6, 10] 216.13 <new_layer_p> rzz [1, 7] 1.33 <new_layer_p> rxx [0, 1] 1.27 <new_layer_p> rzz [2, 10] 198.40 <new_layer_p> ryy [2, 8] 1.32 <new_layer_p> rxx [2, 10] 198.72 <new_layer_p> rxx [3, 7] 1.32 <new_layer_p> rxx [1, 10] 1.36 <new_layer_p> rzz [3, 7] 1.23 <new_layer_p> rxx [3, 13] 1.37 <new_layer_p> rzz [3, 4] 1.18 <new_layer_p> rxx [1, 14] 201.86 <new_layer_p> rzz [0, 11] 1.32 <new_layer_p> rzz [5, 8] 1.23 <new_layer_p> rzz [9, 12] 216.22 <new_layer_p> ryy [8, 9] 1.39 <new_layer_p> rzz [7, 10] 1.45 <new_layer_p> ryy [4, 7] 1.26 <new_layer_p> rzz [1, 5] 1.30 <new_layer_p> ryy [2, 6] 1.28 <new_layer_p> rxx [4, 10] 1.45 <new_layer_p> rzz [6, 14] 18.25 <new_layer_p> rzz [2, 5] -8.88 <new_layer_p> ryy [12, 14] 1.27 <new_layer_p> rxx [2, 5] 190.30 <new_layer_p> rzz [0, 3] 1.33 <new_layer_p> rzz [11, 13] 13.53 <new_layer_p> rzz [4, 10] 1.28 <new_layer_p> rzz [6, 12] 216.33 <new_layer_p> rzz [3, 13] 220.66 <new_layer_p> rxx [0, 1] 1.38 <new_layer_p> rzz [2, 7] 1.39 <new_layer_p> rzz [9, 12] 1.20 <new_layer_p> rzz [3, 5] 1.36 <new_layer_p> ryy [1, 11] 0.02 <new_layer_p> rzz [1, 14] 1.18 <new_layer_p> ryy [6, 14] 1.15 <new_layer_p> rxx [7, 14] 216.20 <new_layer_p> rxx [0, 13] 198.40 <new_layer_p> ryy [0, 7] 1.28 <new_layer_p> rzz [6, 7] 212.11 <new_layer_p> rzz [3, 7] 1.17 <new_layer_p> rzz [0, 5] 1.37 <new_layer_p> rzz [0, 7] 1.22 <new_layer_p> rzz [1, 12] 1.41 <new_layer_p> ryy [0, 13] 1.43 <new_layer_p> rxx [5, 6] 1.32 <new_layer_p> rxx [9, 11] 1.36 <new_layer_p> rxx [1, 9] 1.32 <new_layer_p> rzz [11, 12] 1.36 <new_layer_p> rxx [5, 12] 1.36 <new_layer_p> rzz [9, 12] 1.45 <new_layer_p> rzz [9, 14] 221.80 <new_layer_p> rzz [3, 11] 1.15 <new_layer_p> rzz [8, 10] 1.33 <new_layer_p> ryy [3, 10] 1.27 <new_layer_p> rxx [4, 6] 1.18 <new_layer_p> rxx [3, 7] 1.36 <new_layer_p> rzz [1, 12] 1.32 <new_layer_p> rzz [0, 10] 1.37 <new_layer_p> rzz [0, 8] 1.33 <new_layer_p> rz [12] -10.52 <new_layer_p> rzz [8, 11] 1.27 <new_layer_p> rzz [2, 6] 227.79 <new_layer_p> rzz [0, 6] 1.36 <new_layer_p> rzz [4, 8] 1.32 <new_layer_p> ryy [1, 9] 1.42 <new_layer_p> rzz [4, 5] 1.26 <new_layer_p> rzz [8, 9] 1.33 <new_layer_p> ryy [0, 10] 1.19 <new_layer_p> rzz [7, 11] 1.35 <new_layer_p> rzz [10, 11] 1.37 <new_layer_p> rxx [5, 6] 1.36 <new_layer_p> rzz [2, 13] 1.55 <new_layer_p> rxx [0, 11] 1.25 <new_layer_p> ryy [7, 14] 1.35 <new_layer_p> rxx [11, 14] 1.38 <new_layer_p> ryy [6, 9] 1.17 <new_layer_p> rzz [7, 10] 1.37 <new_layer_p> rzz [0, 4] 1.28 <new_layer_p> rzz [7, 14] 1.25 <new_layer_p> rxx [4, 11] 1.26 <new_layer_p> rzz [4, 8] 1.37 <new_layer_p> rxx [6, 9] 1.32 <new_layer_p> rxx [0, 1] 1.45 <new_layer_p> ryy [12, 14] 1.28 <new_layer_p> rzz [3, 11] 1.36 <new_layer_p> rxx [8, 9] 200.98 <new_layer_p> rzz [1, 7] 1.27 <new_layer_p> rxx [1, 2] 1.28 <new_layer_p> rzz [5, 11] 1.37 <new_layer_p> rxx [7, 12] 1.24 <new_layer_p> ryy [7, 14] 1.36 <new_layer_p> rzz [0, 7] 199.81 <new_layer_p> rzz [0, 8] 1.36 <new_layer_p> rzz [0, 6] 1.10 <new_layer_p> ryy [9, 14] 1.24 <new_layer_p> ryy [2, 10] 1.39 <new_layer_p> rzz [0, 8] 1.09 <new_layer_p> rzz [0, 4] 1.45 <new_layer_p> rxx [13, 14] 1.45 <new_layer_p> ryy [5, 14] 1.32 <new_layer_p> rzz [5, 12] 1.28 <new_layer_p> rzz [2, 4] 1.32 <new_layer_p> rzz [9, 10] 1.32 <new_layer_p> rzz [2, 5] 216.36 <new_layer_p> rxx [3, 13] 1.32 <new_layer_p> rzz [2, 13] 1.38 <new_layer_p> rzz [9, 11] 1.23 <new_layer_p> rzz [3, 11] -0.04 <new_layer_p> rzz [1, 7] 1.40 <new_layer_p> ryy [0, 7] 1.28 <new_layer_p> rxx [10, 11] 1.27 <new_layer_p> rzz [5, 9] 1.32 <new_layer_p> rzz [1, 14] 1.35 <new_layer_p> rzz [4, 10] 1.45 <new_layer_p> rzz [1, 8] 1.17 <new_layer_p> rzz [6, 14] 1.32 <new_layer_p> rxx [6, 9] 1.41 <new_layer_p> ryy [1, 9] 1.28 <new_layer_p> rzz [10, 11] 1.32 <new_layer_p> rzz [1, 7] 1.31 <new_layer_p> rzz [2, 5] 1.47 <new_layer_p> ryy [0, 8] 1.40 <new_layer_p> ryy [0, 1] 1.16 <new_layer_p> ryy [9, 10] 1.45 <new_layer_p> rzz [2, 11] 1.36 <new_layer_p> rzz [12, 13] 182.59 <new_layer_p> rxx [4, 7] 1.31 <new_layer_p> ryy [3, 8] 1.20 <new_layer_p> rxx [3, 9] 1.25 <new_layer_p> ryy [1, 7] 1.36 <new_layer_p> rzz [6, 12] 1.40 <new_layer_p> rzz [4, 5] 1.29 <new_layer_p> rzz [4, 11] 1.36 <new_layer_p> rxx [4, 10] 1.17 <new_layer_p> ryy [6, 9] 1.31 <new_layer_p> rzz [2, 3] 1.16 <new_layer_p> rxx [0, 3] 1.33 <new_layer_p> rxx [0, 14] 1.33 <new_layer_p> ryy [1, 6] 1.55 <new_layer_p> ryy [6, 7] 1.45 <new_layer_p> rzz [9, 10] 1.17 <new_layer_p> rxx [5, 10] 1.26 <new_layer_p> rzz [2, 8] 1.39 <new_layer_p> rxx [0, 9] 1.33 <new_layer_p> rzz [5, 8] 0.02 <new_layer_p> rxx [4, 9] 1.33 <new_layer_p> ryy [0, 12] 1.38 <new_layer_p> rxx [0, 8] 1.36 <new_layer_p> rzz [0, 5] 1.50 <new_layer_p> rzz [2, 7] 1.39 <new_layer_p> rzz [3, 4] 1.33 <new_layer_p> rzz [0, 12] 1.36 <new_layer_p> rzz [0, 8] 1.33 <new_layer_p> rxx [4, 10] 1.36 <new_layer_p> rzz [3, 6] 1.43 <new_layer_p> ryy [5, 11] 1.23 <new_layer_p> rzz [3, 12] 3.14 <new_layer_p> rxx [0, 4] 158.90 <new_layer_p> rzz [11, 14] 1.28 <new_layer_p> ryy [3, 10] 1.32 <new_layer_p> rzz [3, 12] 1.49 <new_layer_p> rzz [10, 12] 1.28 <new_layer_p> rzz [0, 6] 1.17 <new_layer_p> rzz [0, 11] 212.45 <new_layer_p> rzz [4, 10] 1.41 <new_layer_p> rxx [5, 12] 1.27 <new_layer_p> rzz [1, 9] 1.32 <new_layer_p> rzz [10, 13] 1.19 <new_layer_p> rxx [4, 11] 0.02 <new_layer_p> ryy [0, 13] 1.25 <new_layer_p> rzz [2, 13] 1.49 <new_layer_p> rxx [0, 9] 202.22 <new_layer_p> rxx [0, 8] 1.73 <new_layer_p> rzz [5, 11] 218.65 <new_layer_p> rzz [2, 4] 1.25 <new_layer_p> rzz [2, 9] 1.25 <new_layer_p> ryy [4, 5] 1.36 <new_layer_p> rzz [7, 12] 1.31 <new_layer_p> rzz [9, 11] 1.25 <new_layer_p> rzz [6, 11] 1.28 <new_layer_p> rxx [7, 8] 1.33 <new_layer_p> rzz [10, 11] 1.40 <new_layer_p> rzz [0, 10] 1.42 <new_layer_p> ryy [3, 8] 1.26 <new_layer_p> rxx [1, 2] 1.28 <new_layer_p> rxx [9, 13] 0.69 <new_layer_p> rzz [4, 11] 1.41 <new_layer_p> rzz [3, 7] 1.32 <new_layer_p> rxx [2, 3] 1.31 <new_layer_p> rxx [2, 14] 1.28 <new_layer_p> rzz [9, 11] 1.50 <new_layer_p> rzz [6, 11] 214.59 <new_layer_p> rxx [3, 12] 1.28 <new_layer_p> rzz [1, 14] 1.36 <new_layer_p> rzz [4, 8] 1.32 <new_layer_p> rzz [8, 11] 1.43 <new_layer_p> rzz [3, 14] 1.32 <new_layer_p> ryy [0, 4] 1.39 <new_layer_p> rzz [7, 11] 3.14 <new_layer_p> rzz [5, 9] 1.32 <new_layer_p> rzz [5, 6] 1.39 <new_layer_p> ryy [9, 11] 1.40 <new_layer_p> rxx [1, 9] 1.38 <new_layer_p> ryy [4, 13] 1.17 <new_layer_p> rxx [10, 12] 1.33 <new_layer_p> rzz [3, 7] 1.32 <new_layer_p> rzz [0, 8] 1.27 <new_layer_p> rzz [0, 5] 1.38 <new_layer_p> rxx [6, 14] 1.48 <new_layer_p> rzz [3, 8] 1.25 <new_layer_p> rxx [13, 14] 0.02 <new_layer_p> rxx [13, 14] 0.01 <new_layer_p> rxx [4, 11] 1.41 <new_layer_p> rzz [2, 9] 1.19 <new_layer_p> rxx [7, 13] 1.28 <new_layer_p> rxx [2, 7] 1.40 <new_layer_p> ryy [8, 9] 210.90 <new_layer_p> rzz [0, 7] 1.28 <new_layer_p> rxx [7, 9] 1.48 <new_layer_p> rzz [3, 4] 1.36 <new_layer_p> rzz [10, 13] 1.38 <new_layer_p> rxx [5, 11] 1.36 <new_layer_p> rzz [2, 4] 1.34 <new_layer_p> rzz [3, 12] 1.33 <new_layer_p> rxx [1, 13] 1.35 <new_layer_p> rzz [3, 8] 1.16 <new_layer_p> rzz [4, 5] 1.36 <new_layer_p> ryy [3, 11] 1.28 <new_layer_p> rzz [6, 7] 1.27 <new_layer_p> ryy [2, 5] 209.58 <new_layer_p> rxx [8, 10] 1.33 <new_layer_p> rxx [2, 5] 209.58 <new_layer_p> ryy [1, 14] 1.27 <new_layer_p> ryy [6, 7] 1.27 <new_layer_p> rxx [6, 7] 1.38 <new_layer_p> rzz [11, 12] 1.28 <new_layer_p> ryy [8, 11] 1.38 <new_layer_p> ryy [1, 11] 1.15 <new_layer_p> rzz [6, 7] 1.37 <new_layer_p> ryy [4, 12] 1.29 <new_layer_p> rzz [6, 8] 0.02 <new_layer_p> rzz [2, 7] 1.33 <new_layer_p> rzz [2, 8] 1.12 <new_layer_p> rxx [4, 6] 1.39 <new_layer_p> ryy [0, 13] 1.27 <new_layer_p> ryy [2, 14] 1.33 <new_layer_p> rxx [2, 9] 1.19 <new_layer_p> ryy [5, 6] 1.24 <new_layer_p> rzz [6, 8] 1.33 <new_layer_p> rxx [9, 10] 11.86 <new_layer_p> rxx [2, 14] 1.32 <new_layer_p> rzz [6, 13] 1.35 <new_layer_p> rzz [1, 2] 1.37 <new_layer_p> rzz [9, 14] 1.33 <new_layer_p> ryy [0, 1] 1.18 <new_layer_p> rzz [4, 5] 1.11 <new_layer_p> rzz [3, 8] 1.17 <new_layer_p> rzz [0, 9] 1.42 <new_layer_p> rxx [3, 5] 1.35 <new_layer_p> rzz [5, 10] 1.39 <new_layer_p> rxx [3, 11] 1.27 <new_layer_p> rxx [7, 8] 1.33 <new_layer_p> rxx [4, 13] 1.26 <new_layer_p> rzz [5, 8] 1.23 <new_layer_p> rzz [3, 11] 1.18 <new_layer_p> rzz [3, 7] 1.20 <new_layer_p> rzz [3, 7] 1.43 <new_layer_p> rzz [3, 7] 1.40 <new_layer_p> ryy [6, 8] 1.28 <new_layer_p> rzz [9, 14] 1.39 <new_layer_p> rzz [5, 8] 1.35 <new_layer_p> rzz [12, 13] 1.45 <new_layer_p> rxx [8, 10] 1.33 <new_layer_p> rxx [13, 14] 206.76 <new_layer_p> rzz [9, 11] 1.33 <new_layer_p> rzz [0, 12] 1.30 <new_layer_p> rzz [10, 11] 1.28 <new_layer_p> rzz [6, 8] 1.29 <new_layer_p> rzz [3, 14] 1.15 <new_layer_p> rxx [7, 8] 1.32 <new_layer_p> rxx [3, 5] 1.07 <new_layer_p> ryy [5, 12] 1.37 <new_layer_p> rzz [3, 7] 1.25 <new_layer_p> rzz [1, 8] 1.27 <new_layer_p> rzz [1, 11] 1.25 <new_layer_p> rxx [6, 12] 214.33 <new_layer_p> rzz [9, 13] 1.49 <new_layer_p> ryy [0, 9] <new_layer_p> rzz [3, 13] 1.39 <new_layer_p> rxx [2, 11] 1.36 <new_layer_p> rxx [7, 14] 1.17 <new_layer_p> rxx [0, 4] 1.29 <new_layer_p> rzz [1, 11] 1.38 <new_layer_p> ryy [2, 11] 1.46 <new_layer_p> rxx [3, 4] 1.36 <new_layer_p> rxx [3, 5] 1.13 <new_layer_p> rzz [10, 14] 1.17 <new_layer_p> rzz [12, 13] 1.36 <new_layer_p> rzz [0, 1] 1.05 <new_layer_p> rzz [0, 9] 1.43 <new_layer_p> rxx [0, 1] 1.19 <new_layer_p> rzz [4, 8] 1.32 <new_layer_p> rzz [6, 13] 1.27 <new_layer_p> rxx [1, 12] 1.40 <new_layer_p> rzz [6, 9] 1.45 <new_layer_p> ryy [9, 14] 1.41 <new_layer_p> rxx [5, 10] 1.47 <new_layer_p> ryy [4, 11] 1.32 <new_layer_p> rzz [3, 6] 1.26 <new_layer_p> rzz [4, 9] 1.41 <new_layer_p> rxx [5, 6] 1.43 <new_layer_p> ryy [2, 11] 1.38 <new_layer_p> rxx [3, 9] 1.45 <new_layer_p> rxx [12, 13] 1.37 <new_layer_p> rzz [4, 12] 1.33 <new_layer_p> rzz [7, 14] 1.33 <new_layer_p> ryy [3, 13] 1.32 <new_layer_p> rzz [8, 12] 1.45 <new_layer_p> rxx [8, 11] 1.33 <new_layer_p> rzz [1, 8] 1.28 <new_layer_p> rxx [11, 13] 1.55 <new_layer_p> rzz [4, 13] 1.35 <new_layer_p> rzz [5, 8] 1.28 <new_layer_p> rzz [4, 9] 1.28 <new_layer_p> ryy [2, 7] 1.39 <new_layer_p> rzz [1, 8] 1.27 <new_layer_p> rxx [11, 13] 1.36 <new_layer_p> ryy [0, 5] 1.19 <new_layer_p> rzz [1, 4] 1.15 <new_layer_p> rzz [1, 12] 1.36 <new_layer_p> rxx [8, 12] 1.45 <new_layer_p> ryy [3, 4] 222.21 <new_layer_p> ryy [12, 13] 1.33 <new_layer_p> rzz [6, 14] 1.39 <new_layer_p> rxx [0, 7] 1.36 <new_layer_p> ryy [1, 6] 1.30 <new_layer_p> rxx [13, 14] 1.32 <new_layer_p> rzz [0, 11] 212.09 <new_layer_p> rxx [5, 10] 1.28 <new_layer_p> rzz [1, 7] 1.31 <new_layer_p> rzz [3, 9] 1.27 <new_layer_p> rxx [0, 1] 1.28 <new_layer_p> rxx [1, 13] 1.36 <new_layer_p> ryy [0, 5] 1.36 <new_layer_p> ryy [5, 6] 1.45 <new_layer_p> rzz [5, 13] 1.17 <new_layer_p> rxx [1, 12] 1.40 <new_layer_p> ryy [2, 7] 1.28 <new_layer_p> rzz [1, 9] 1.32 <new_layer_p> ryy [1, 9] 1.25 <new_layer_p> rzz [3, 7] 1.32 <new_layer_p> rzz [2, 7] 1.39 <new_layer_p> rxx [7, 14] 1.37 <new_layer_p> rxx [8, 14] 1.28 <new_layer_p> ryy [7, 10] 1.45 <new_layer_p> rzz [9, 10] 1.27 <new_layer_p> rxx [12, 14] 1.69 <new_layer_p> rzz [6, 13] 1.27 <new_layer_p> rxx [3, 12] 1.35 <new_layer_p> rzz [4, 9] 1.28 <new_layer_p> ryy [0, 4] 1.14 <new_layer_p> ryy [3, 12] 1.36 <new_layer_p> ryy [3, 11] 1.33 <new_layer_p> rxx [9, 12] 1.45 <new_layer_p> ryy [3, 7] 1.17 <new_layer_p> rzz [3, 11] 1.29 <new_layer_p> ryy [1, 2] 1.28 <new_layer_p> rzz [11, 14] 211.83 <new_layer_p> rzz [3, 6] 1.27 <new_layer_p> ryy [9, 10] 1.45 <new_layer_p> rxx [12, 14] 3.14 <new_layer_p> rzz [4, 7] 1.36 <new_layer_p> rxx [3, 11] 1.35 <new_layer_p> rzz [9, 14] 3.14 <new_layer_p> rxx [2, 13] 1.17 <new_layer_p> rzz [7, 9] 1.32 <new_layer_p> rxx [2, 11] 1.46 <new_layer_p> ryy [12, 14] 0.01 <new_layer_p> rzz [1, 10] 1.36 <new_layer_p> rzz [7, 13] 1.35 <new_layer_p> ryy [1, 10] 1.33 <new_layer_p> rzz [7, 14] 1.25 <new_layer_p> rzz [8, 14] 1.11 <new_layer_p> rxx [3, 6] 1.33 <new_layer_p> rzz [9, 12] 1.28 <new_layer_p> rxx [1, 12] 1.38 <new_layer_p> rzz [3, 14] 1.32 <new_layer_p> rzz [3, 6] 1.11 <new_layer_p> rzz [1, 2] 1.43 <new_layer_p> rzz [3, 8] 1.33 <new_layer_p> rzz [1, 14] 1.17 <new_layer_p> rzz [3, 5] 1.28 <new_layer_p> rzz [3, 4] 1.33 <new_layer_p> rxx [8, 9] 1.21 <new_layer_p> rzz [6, 7] 1.30 <new_layer_p> rxx [3, 12] 1.12 <new_layer_p> rxx [4, 11] 216.41 <new_layer_p> ryy [6, 12] 1.36 <new_layer_p> rzz [7, 9] 1.38 <new_layer_p> rxx [1, 5] 1.25 <new_layer_p> rxx [2, 4] 1.05 <new_layer_p> rzz [2, 13] 1.30 <new_layer_p> rzz [4, 5] 1.14 <new_layer_p> rzz [4, 14] 1.19 <new_layer_p> ryy [3, 10] 1.32 <new_layer_p> rzz [0, 12] 1.30 <new_layer_p> ryy [1, 11] 1.10 <new_layer_p> ryy [4, 12] 1.39 <new_layer_p> ryy [10, 12] 1.40 <new_layer_p> ryy [0, 8] 1.45 <new_layer_p> ryy [5, 9] 0.02 <new_layer_p> ryy [3, 7] 1.40 <new_layer_p> rzz [5, 12] 214.05 <new_layer_p> ryy [8, 12] 1.33 <new_layer_p> rxx [4, 8] 1.38 <new_layer_p> rzz [6, 14] 1.43 <new_layer_p> ryy [3, 9] 1.32 <new_layer_p> rxx [2, 10] 1.45 <new_layer_p> rxx [8, 9] 1.17 <new_layer_p> rzz [4, 11] 1.32 <new_layer_p> rzz [3, 12] 1.33 <new_layer_p> rxx [8, 14] 1.16 <new_layer_p> rxx [1, 5] 1.41 <new_layer_p> ryy [4, 10] 1.33 <new_layer_p> rzz [2, 12] 1.33 <new_layer_p> rxx [3, 4] 1.32 <new_layer_p> rxx [4, 13] 1.28 <new_layer_p> ryy [7, 14] 214.60 <new_layer_p> ryy [6, 10] 1.19 <new_layer_p> ryy [0, 8] 1.47 <new_layer_p> rxx [0, 10] 1.36 <new_layer_p> ryy [0, 1] 1.27 <new_layer_p> ryy [3, 8] 1.17 <new_layer_p> rzz [2, 5] 1.33 <new_layer_p> rzz [1, 2] 1.36 <new_layer_p> rzz [0, 8] 1.27 <new_layer_p> rzz [2, 11] 1.26 <new_layer_p> rxx [1, 12] 1.39 <new_layer_p> rzz [1, 10] 1.28 <new_layer_p> ryy [0, 11] 1.16 <new_layer_p> rxx [3, 14] 1.36 <new_layer_p> rzz [3, 7] 1.28 <new_layer_p> rzz [4, 10] 1.21 <new_layer_p> ryy [3, 13] 1.19 <new_layer_p> ryy [0, 7] 1.28 <new_layer_p> rzz [2, 8] 1.40 <new_layer_p> rzz [3, 6] 1.37 <new_layer_p> ryy [3, 7] 1.33 <new_layer_p> ryy [10, 11] 1.38 <new_layer_p> ryy [3, 7] 1.43 <new_layer_p> rzz [1, 14] 1.28 <new_layer_p> rxx [0, 4] 1.27 <new_layer_p> rzz [12, 13] 198.40 <new_layer_p> rxx [3, 14] 1.17 <new_layer_p> rzz [6, 7] 1.32 <new_layer_p> rxx [3, 8] 1.28 <new_layer_p> rzz [1, 12] 1.17 <new_layer_p> rxx [0, 1] 1.41 <new_layer_p> rzz [0, 1] 1.41 <new_layer_p> ryy [2, 11] 1.05 <new_layer_p> rxx [1, 8] 1.27 <new_layer_p> rxx [0, 4] 1.29 <new_layer_p> rzz [0, 14] 1.17 <new_layer_p> rzz [2, 11] 1.46 <new_layer_p> ryy [7, 10] 1.28 <new_layer_p> rzz [1, 3] 1.36 <new_layer_p> rzz [0, 13] 1.35 <new_layer_p> rzz [3, 8] 1.39 <new_layer_p> ryy [3, 5] 1.17 <new_layer_p> rzz [2, 10] 1.16 <new_layer_p> rzz [5, 10] 1.31 <new_layer_p> ryy [1, 12] 1.37 <new_layer_p> rzz [0, 8] 1.36 <new_layer_p> ryy [0, 5] 1.33 <new_layer_p> ryy [9, 14] 1.35 <new_layer_p> rzz [4, 10] 1.26 <new_layer_p> ryy [0, 13] 1.37 <new_layer_p> rzz [6, 12] 1.29 <new_layer_p> rxx [3, 8] 1.36 <new_layer_p> ryy [6, 12] 1.36 <new_layer_p> ryy [8, 9] 198.11 <new_layer_p> rzz [6, 11] 1.42 <new_layer_p> rzz [6, 12] 1.33 <new_layer_p> ryy [2, 13] 1.28 <new_layer_p> ryy [3, 12] 1.32 <new_layer_p> ryy [3, 12] 1.24 <new_layer_p> ryy [3, 13] 1.29 <new_layer_p> ryy [2, 10] 1.36 <new_layer_p> rzz [11, 14] 1.26 <new_layer_p> rzz [3, 13] 1.34 <new_layer_p> rzz [3, 5] 1.37 <new_layer_p> rxx [1, 14] 1.32 <new_layer_p> rzz [11, 14] 1.36 <new_layer_p> rxx [3, 12] 1.28 <new_layer_p> rzz [2, 5] 1.17 <new_layer_p> rxx [4, 11] 1.36 <new_layer_p> rzz [2, 3] 1.36 <new_layer_p> rzz [1, 12] 1.31 <new_layer_p> ryy [3, 4] 1.17 <new_layer_p> rzz [6, 12] 1.28 <new_layer_p> rzz [3, 14] 1.16 <new_layer_p> rzz [0, 4] 1.21 <new_layer_p> rxx [4, 7] 3.14 <new_layer_p> rzz [7, 8] 1.33 <new_layer_p> rxx [4, 9] 1.32 <new_layer_p> ryy [9, 10] 1.32 <new_layer_p> rzz [4, 7] 1.40 <new_layer_p> ryy [3, 11] 1.37 <new_layer_p> rzz [4, 11] 1.32 <new_layer_p> rzz [4, 10] 1.28 <new_layer_p> rzz [3, 8] 202.65 <new_layer_p> rzz [1, 9] 1.33 <new_layer_p> rzz [0, 4] 1.45 <new_layer_p> rzz [8, 14] 1.39 <new_layer_p> rxx [2, 13] 1.25 <new_layer_p> rzz [7, 13] 1.28 <new_layer_p> rzz [9, 10] 1.10 <new_layer_p> rzz [3, 7] 1.33 <new_layer_p> rxx [2, 3] 1.37 <new_layer_p> ryy [3, 12] 1.25 <new_layer_p> rzz [8, 12] 1.30 <new_layer_p> rzz [4, 9] 1.05 <new_layer_p> rzz [3, 7] 0.02 <new_layer_p> rxx [3, 7] 1.27 <new_layer_p> rzz [4, 9] 1.28 <new_layer_p> rzz [4, 8] 1.32 <new_layer_p> rxx [2, 13] 1.36 <new_layer_p> rzz [3, 9] 1.36 <new_layer_p> ryy [9, 13] 1.37 <new_layer_p> rzz [8, 14] 1.30 <new_layer_p> ryy [3, 5] 1.36 <new_layer_p> rxx [6, 14] 1.37 <new_layer_p> ryy [10, 14] 1.23 <new_layer_p> rzz [4, 5] 1.45 <new_layer_p> rzz [0, 9] 1.39 <new_layer_p> rzz [2, 12] 1.33 <new_layer_p> ryy [11, 14] 1.36 <new_layer_p> rzz [1, 12] 1.32 <new_layer_p> rzz [3, 6] 1.33 <new_layer_p> ryy [9, 14] 1.39 <new_layer_p> rxx [3, 12] 1.25 <new_layer_p> rxx [10, 12] 1.33 <new_layer_p> rzz [2, 10] 198.40 <new_layer_p> rzz [6, 14] 1.28 <new_layer_p> rzz [10, 14] 1.29 <new_layer_p> rzz [1, 6] 1.17 <new_layer_p> rxx [3, 7] 1.32 <new_layer_p> rxx [1, 2] 1.05 <new_layer_p> rxx [4, 10] 1.17 <new_layer_p> rzz [4, 14] 1.39 <new_layer_p> rxx [5, 9] 1.39 <new_layer_p> ryy [3, 13] 1.27 <new_layer_p> rxx [3, 9] 1.47 <new_layer_p> rxx [13, 14] 1.48 <new_layer_p> ryy [2, 7] 1.25 <new_layer_p> ryy [0, 4] 1.18 <new_layer_p> rxx [2, 10] 1.05 <new_layer_p> ryy [9, 12] 1.45 <new_layer_p> ryy [11, 13] 1.17 <new_layer_p> rzz [4, 13] 1.33 <new_layer_p> rzz [7, 13] 190.63 <new_layer_p> rzz [5, 13] 1.33 <new_layer_p> rzz [12, 13] 1.33 <new_layer_p> rzz [3, 11] 1.29 <new_layer_p> rxx [0, 14] 1.28 <new_layer_p> ryy [8, 14] 1.37 <new_layer_p> rzz [10, 11] 1.28 <new_layer_p> rxx [0, 5] 1.19 <new_layer_p> rzz [2, 13] 1.37 <new_layer_p> rzz [10, 14] 1.39 <new_layer_p> ryy [3, 14] 1.32 <new_layer_p> rzz [3, 9] 1.47 <new_layer_p> rxx [8, 11] 1.32 <new_layer_p> rxx [2, 12] 1.33 <new_layer_p> rzz [3, 13] 0.79 <new_layer_p> rzz [10, 14] 1.28 <new_layer_p> rzz [3, 12] 1.31 <new_layer_p> ryy [0, 8] 1.16 <new_layer_p> rzz [1, 8] 1.28 <new_layer_p> rxx [3, 13] 1.20 <new_layer_p> rzz [3, 4] 1.33 <new_layer_p> ryy [4, 13] 1.39 <new_layer_p> rzz [9, 11] 1.32 <new_layer_p> rzz [3, 6] 1.39 <new_layer_p> rzz [4, 11] 1.36 <new_layer_p> rxx [2, 5] 3.14 <new_layer_p> rxx [10, 12] 1.36 <new_layer_p> rxx [1, 12] 1.36 <new_layer_p> rzz [0, 14] 1.39 <new_layer_p> rzz [8, 14] 1.38 <new_layer_p> ryy [5, 8] 1.32 <new_layer_p> rxx [9, 10] 1.40 <new_layer_p> rxx [7, 14] 1.37 <new_layer_p> ryy [8, 11] 1.17 <new_layer_p> ryy [8, 9] 190.69 <new_layer_p> rzz [0, 8] 1.36 <new_layer_p> rzz [3, 8] 1.39 <new_layer_p> ryy [8, 12] 1.27 <new_layer_p> rzz [2, 4] 1.49 <new_layer_p> ryy [1, 7] 1.36 <new_layer_p> ryy [3, 12] 1.28 <new_layer_p> rzz [4, 12] 1.33 <new_layer_p> rzz [5, 6] 1.39 <new_layer_p> rzz [3, 9] 1.33 <new_layer_p> rxx [8, 11] 1.39 <new_layer_p> rzz [4, 11] 1.36 <new_layer_p> rzz [7, 14] 1.38 <new_layer_p> rzz [2, 5] 1.31 <new_layer_p> rxx [8, 10] 199.64 <new_layer_p> rzz [9, 13] 1.33 <new_layer_p> ryy [1, 7] 1.24 <new_layer_p> rzz [2, 4] 0.01 <new_layer_p> rzz [4, 9] 1.24 <new_layer_p> rxx [7, 13] 1.38 <new_layer_p> rzz [1, 8] 1.17 <new_layer_p> rzz [0, 5] 1.25 <new_layer_p> rzz [1, 6] 1.49 <new_layer_p> rzz [6, 13] 1.18 <new_layer_p> ryy [8, 10] 1.45 <new_layer_p> rzz [6, 14] 1.45 <new_layer_p> rzz [2, 14] 1.25 <new_layer_p> rxx [2, 8] 1.33 <new_layer_p> rxx [1, 10] 1.25 <new_layer_p> rxx [1, 13] 1.55 <new_layer_p> ryy [7, 13] 1.18 <new_layer_p> rzz [1, 4] 1.36 <new_layer_p> rzz [5, 11] 1.32 <new_layer_p> rxx [7, 14] 1.35 <new_layer_p> rzz [3, 12] 1.38 <new_layer_p> ryy [3, 7] 1.17 <new_layer_p> ryy [2, 11] 1.28 <new_layer_p> rxx [6, 8] 1.27 <new_layer_p> rzz [9, 14] 1.38 <new_layer_p> rxx [3, 6] 0.01 <new_layer_p> rzz [6, 9] 1.17 <new_layer_p> rxx [5, 14] 1.33 <new_layer_p> rzz [10, 12] 1.36 <new_layer_p> rzz [6, 8] 1.25 <new_layer_p> rxx [6, 12] 1.28 <new_layer_p> rzz [5, 9] 1.27 <new_layer_p> rzz [2, 11] 1.36 <new_layer_p> rzz [3, 5] 1.36 <new_layer_p> ryy [3, 14] 1.17 <new_layer_p> rzz [0, 9] 1.45 <new_layer_p> rxx [12, 13] 1.35 <new_layer_p> rxx [3, 14] 1.34 <new_layer_p> ryy [7, 13] 1.27 <new_layer_p> rxx [9, 11] 1.36 <new_layer_p> rzz [6, 10] 1.33 <new_layer_p> rxx [10, 11] 1.33 <new_layer_p> ryy [5, 11] 1.28 <new_layer_p> rxx [11, 14] 214.52 <new_layer_p> rzz [0, 11] 1.32 <new_layer_p> ryy [3, 13] 1.27 <new_layer_p> rxx [0, 2] 1.36 <new_layer_p> ryy [1, 8] 1.28 <new_layer_p> rzz [3, 12] 209.58 <new_layer_p> rzz [1, 5] 1.33 <new_layer_p> rzz [11, 14] 1.31 <new_layer_p> rxx [3, 6] 1.37 <new_layer_p> rzz [5, 11] 1.32 <new_layer_p> rzz [3, 6] 1.27 <new_layer_p> rzz [10, 13] 1.33 <new_layer_p> rxx [0, 5] 1.33 <new_layer_p> rxx [2, 4] 1.37 <new_layer_p> rxx [4, 12] 1.28 <new_layer_p> rxx [0, 7] 1.28 <new_layer_p> ryy [6, 13] 211.70 <new_layer_p> rzz [3, 11] 1.32 <new_layer_p> rzz [3, 14] 1.15 <new_layer_p> rxx [2, 7] 1.34 <new_layer_p> rzz [3, 13] 1.32 <new_layer_p> rzz [4, 13] 1.15 <new_layer_p> ryy [2, 4] 3.14 <new_layer_p> rzz [3, 12] 1.32 <new_layer_p> rxx [8, 11] 1.32 <new_layer_p> rxx [2, 14] 1.36 <new_layer_p> rxx [3, 12] 1.17 <new_layer_p> rxx [2, 9] 1.37 <new_layer_p> ryy [0, 10] 1.33 <new_layer_p> rxx [3, 5] 1.39 <new_layer_p> rzz [2, 7] 1.32 <new_layer_p> rzz [9, 10] 1.27 <new_layer_p> rzz [10, 13] 1.39 <new_layer_p> rzz [5, 6] 1.39 <new_layer_p> rxx [3, 7] 1.43 <new_layer_p> rxx [9, 10] 1.32 <new_layer_p> rzz [0, 7] 1.05 <new_layer_p> rxx [2, 3] 1.16 <new_layer_p> rxx [3, 7] 1.33 <new_layer_p> rzz [7, 14] 1.38 <new_layer_p> rzz [2, 5] 1.32 <new_layer_p> rxx [0, 4] 1.29 <new_layer_p> rzz [8, 14] 1.41 <new_layer_p> ryy [8, 12] -8.06 <new_layer_p> rxx [0, 4] 1.39 <new_layer_p> rzz [4, 11] 1.36 <new_layer_p> rzz [3, 5] 1.26 <new_layer_p> rxx [3, 10] 0.00 <new_layer_p> rzz [4, 5] 1.36 <new_layer_p> ryy [0, 9] 1.48 <new_layer_p> rzz [7, 8] 1.33 <new_layer_p> rxx [7, 14] 1.38 <new_layer_p> rzz [10, 11] 1.33 <new_layer_p> rzz [4, 10] 1.33 <new_layer_p> rzz [10, 11] 1.35 <new_layer_p> rxx [1, 7] 1.32 <new_layer_p> rxx [6, 8] 1.28 <new_layer_p> rxx [8, 11] 1.39 <new_layer_p> rzz [2, 9] 1.45 <new_layer_p> ryy [4, 8] 1.28 <new_layer_p> rxx [8, 10] 1.28 <new_layer_p> rxx [5, 8] 1.12 <new_layer_p> rxx [3, 12] 1.17 <new_layer_p> rzz [8, 12] 1.20 <new_layer_p> rzz [5, 8] 1.28 <new_layer_p> ryy [6, 10] 1.36 <new_layer_p> rxx [5, 7] 1.17 <new_layer_p> rxx [4, 11] 1.36 <new_layer_p> rzz [3, 13] 1.33 <new_layer_p> rzz [9, 11] 1.33 <new_layer_p> rzz [5, 6] 197.62 <new_layer_p> rzz [2, 4] 1.43 <new_layer_p> rxx [12, 13] 1.35 <new_layer_p> rxx [3, 12] 1.17 <new_layer_p> ryy [4, 5] 1.45 <new_layer_p> rxx [3, 8] 1.40 <new_layer_p> rzz [12, 14] 1.28 <new_layer_p> rzz [4, 5] 1.28 <new_layer_p> rzz [3, 13] 1.32 <new_layer_p> rzz [9, 10] 1.32 <new_layer_p> ryy [0, 2] 1.33 <new_layer_p> rxx [0, 1] 1.27 <new_layer_p> rxx [2, 11] 1.27 <new_layer_p> rzz [6, 9] 1.40 <new_layer_p> rzz [4, 6] 1.39 <new_layer_p> rxx [0, 3] 1.33 <new_layer_p> rzz [0, 8] 1.27 <new_layer_p> rxx [1, 13] 1.36 <new_layer_p> rzz [1, 4] 1.35 <new_layer_p> rxx [4, 10] 1.16 <new_layer_p> rzz [2, 8] 1.39 <new_layer_p> rxx [3, 12] 1.28 <new_layer_p> rzz [1, 12] 1.33 <new_layer_p> ryy [9, 14] 1.31 <new_layer_p> rzz [2, 10] 1.16 <new_layer_p> rzz [1, 10] 1.33 <new_layer_p> rzz [4, 14] 1.28 <new_layer_p> rxx [3, 7] 1.33 <new_layer_p> rzz [10, 11] 1.09 <new_layer_p> rxx [9, 10] 1.45 <new_layer_p> rzz [1, 14] 1.36 <new_layer_p> rzz [2, 10] 198.40 <new_layer_p> rxx [9, 12] 1.31 <new_layer_p> rzz [0, 6] 1.47 <new_layer_p> rxx [8, 14] 1.36 <new_layer_p> rzz [3, 14] 1.32 <new_layer_p> rzz [0, 5] 1.45 <new_layer_p> rzz [6, 14] 1.39 <new_layer_p> rzz [10, 12] 1.36 <new_layer_p> rxx [13, 14] 0.01 <new_layer_p> ryy [1, 14] 1.23 <new_layer_p> rzz [6, 9] 1.17 <new_layer_p> rxx [0, 10] 1.39 <new_layer_p> rzz [5, 7] 1.55 <new_layer_p> rzz [2, 9] 1.25 <new_layer_p> rzz [2, 10] 1.16 <new_layer_p> rxx [1, 13] 1.38 <new_layer_p> rxx [5, 9] 1.57 <new_layer_p> rzz [8, 14] 1.36 <new_layer_p> rxx [0, 8] 1.28 <new_layer_p> ryy [2, 11] 1.28 <new_layer_p> rzz [11, 13] 1.41 <new_layer_p> rzz [2, 6] 1.35 <new_layer_p> rzz [3, 12] 1.20 <new_layer_p> rxx [2, 12] 1.45 <new_layer_p> rxx [2, 14] 1.39 <new_layer_p> rzz [2, 11] 0.02 <new_layer_p> rzz [6, 14] 1.36 <new_layer_p> rzz [4, 11] 1.25 <new_layer_p> rzz [9, 11] 1.28 <new_layer_p> rzz [4, 9] 1.45 <new_layer_p> rzz [0, 5] 1.39 <new_layer_p> ryy [7, 8] 1.37 <new_layer_p> rzz [1, 3] 1.41 <new_layer_p> rzz [4, 6] 1.19 <new_layer_p> rzz [2, 10] 1.32 <new_layer_p> ryy [0, 8] 1.33 <new_layer_p> rxx [4, 5] 1.36 <new_layer_p> ryy [5, 8] 1.28 <new_layer_p> rzz [0, 1] 1.37 <new_layer_p> ryy [2, 7] 0.01 <new_layer_p> rxx [0, 7] 1.18 <new_layer_p> rxx [3, 7] 1.27 <new_layer_p> rzz [8, 14] 1.27 <new_layer_p> rxx [5, 10] 1.13 <new_layer_p> ryy [1, 5] 216.45 <new_layer_p> ryy [6, 9] 1.33 <new_layer_p> ryy [2, 9] 1.36 <new_layer_p> rzz [9, 12] 1.18 <new_layer_p> rxx [1, 11] 1.41 <new_layer_p> rzz [12, 13] 1.36 <new_layer_p> rxx [2, 9] 1.39 <new_layer_p> ryy [0, 1] 1.27 <new_layer_p> rzz [0, 10] 1.32 <new_layer_p> ryy [8, 14] 1.33 <new_layer_p> rxx [0, 9] 1.40 <new_layer_p> rxx [4, 14] 1.28 <new_layer_p> rxx [1, 11] 1.31 <new_layer_p> rzz [11, 12] 1.32 <new_layer_p> rzz [0, 4] 1.39 <new_layer_p> rxx [1, 12] 1.17 <new_layer_p> rxx [9, 14] 1.45 <new_layer_p> rxx [3, 12] 1.25 <new_layer_p> ryy [10, 11] 1.34 <new_layer_p> rzz [4, 7] 1.41 <new_layer_p> ryy [7, 14] 1.38 <new_layer_p> rzz [0, 11] 1.38 <new_layer_p> rxx [13, 14] 0.00 <new_layer_p> rzz [7, 13] 1.27 <new_layer_p> rxx [2, 9] 1.33 <new_layer_p> rxx [3, 9] 1.33 <new_layer_p> rzz [9, 11] 1.32 <new_layer_p> rzz [3, 12] 1.25 <new_layer_p> rzz [10, 11] 1.40 <new_layer_p> rxx [0, 7] 1.05 <new_layer_p> rxx [5, 6] 1.17 <new_layer_p> rxx [3, 5] 1.26 <new_layer_p> ryy [0, 6] 1.34 <new_layer_p> rzz [3, 5] (0,8) <new_layer_p> ryy [10, 14] 1.17 <new_layer_p> rxx [0, 5] 1.39 <new_layer_p> rzz [3, 6] 1.33 <new_layer_p> rxx [5, 9] 1.27 <new_layer_p> rzz [2, 7] 1.33 <new_layer_p> rxx [9, 11] 1.36 <new_layer_p> rxx [8, 14] 1.37 <new_layer_p> rxx [2, 10] 198.40 <new_layer_p> rzz [0, 10] 1.38 <new_layer_p> rxx [2, 14] 1.17 <new_layer_p> rzz [0, 9] 1.30 <new_layer_p> rzz [8, 13] 1.35 <new_layer_p> rxx [1, 12] 1.40 <new_layer_p> rzz [8, 14] 1.39 <new_layer_p> rzz [6, 13] 1.18 <new_layer_p> rzz [1, 12] 1.45 <new_layer_p> rzz [13, 14] 207.26 <new_layer_p> rzz [7, 14] 1.38 <new_layer_p> ryy [1, 12] 1.20 <new_layer_p> rxx [10, 13] 1.39 <new_layer_p> rxx [8, 14] 1.38 <new_layer_p> rzz [1, 9] 1.27 <new_layer_p> rzz [7, 14] 1.33 <new_layer_p> rxx [3, 6] 1.43 <new_layer_p> ryy [6, 11] 1.36 <new_layer_p> rzz [3, 4] 1.33 <new_layer_p> rzz [1, 5] 1.32 <new_layer_p> ryy [0, 14] 1.41 <new_layer_p> rzz [6, 8] 1.32 <new_layer_p> rxx [1, 9] 1.38 <new_layer_p> rxx [3, 14] 1.28 <new_layer_p> rzz [3, 9] 1.47 <new_layer_p> rxx [4, 12] 1.35 <new_layer_p> rxx [5, 8] 1.33 <new_layer_p> rzz [0, 3] 1.34 <new_layer_p> rzz [3, 7] 1.33 <new_layer_p> ryy [7, 13] 1.33 <new_layer_p> ryy [6, 7] 1.45 <new_layer_p> rzz [3, 11] 1.28 <new_layer_p> rzz [1, 5] 1.33 <new_layer_p> rzz [1, 5] 1.39 <new_layer_p> rxx [3, 11] 1.45 <new_layer_p> rzz [0, 2] 1.33 <new_layer_p> rxx [4, 13] 1.28 <new_layer_p> rxx [7, 14] 1.37 <new_layer_p> rxx [5, 6] 1.28 <new_layer_p> rzz [6, 11] 1.28 <new_layer_p> rzz [6, 7] 1.26 <new_layer_p> ryy [4, 11] 1.36 <new_layer_p> rzz [1, 2] 1.36 <new_layer_p> rxx [6, 10] 1.47 <new_layer_p> rzz [1, 2] 1.43 <new_layer_p> rxx [3, 6] 0.02 <new_layer_p> rxx [2, 9] 1.33 <new_layer_p> ryy [8, 12] 1.41 <new_layer_p> ryy [2, 13] 1.35 <new_layer_p> rzz [7, 13] 1.39 <new_layer_p> rzz [1, 3] 1.33 <new_layer_p> rxx [1, 14] 1.36 <new_layer_p> rzz [2, 3] 1.28 <new_layer_p> ryy [1, 12] 1.41 <new_layer_p> rzz [8, 11] 1.54 <new_layer_p> rzz [12, 13] 1.34 <new_layer_p> ryy [10, 12] 199.61 <new_layer_p> rzz [2, 13] 1.35 <new_layer_p> rzz [0, 1] 1.41 <new_layer_p> rzz [3, 5] 1.32 <new_layer_p> ryy [5, 9] 1.26 <new_layer_p> rzz [3, 7] 1.33 <new_layer_p> rxx [2, 14] 1.18 <new_layer_p> rzz [9, 10] 1.32 <new_layer_p> rzz [12, 14] 3.14 <new_layer_p> rzz [0, 6] 1.26 <new_layer_p> rzz [3, 5] 1.36 <new_layer_p> rxx [2, 5] 1.33 <new_layer_p> rxx [6, 14] 1.32 <new_layer_p> rxx [4, 10] 1.33 <new_layer_p> rxx [6, 11] 1.12 <new_layer_p> ryy [6, 11] 1.28 <new_layer_p> ryy [2, 3] 1.40 <new_layer_p> rzz [11, 12] 1.11 <new_layer_p> rxx [1, 14] 1.45 <new_layer_p> rxx [4, 9] 1.45 <new_layer_p> rzz [1, 12] 1.38 <new_layer_p> rzz [5, 7] 1.28 <new_layer_p> rzz [1, 11] 1.36 <new_layer_p> rzz [3, 13] 1.41 <new_layer_p> ryy [1, 14] 3.13 <new_layer_p> rzz [3, 12] 1.33 <new_layer_p> ryy [0, 5] 1.38 <new_layer_p> rxx [3, 10] 1.27 <new_layer_p> rzz [0, 5] 3.14 <new_layer_p> rzz [3, 9] 1.33 <new_layer_p> ryy [3, 5] 1.33 <new_layer_p> rxx [2, 12] 1.33 <new_layer_p> ryy [3, 14] 1.17 <new_layer_p> rzz [3, 5] 1.17 <new_layer_p> rxx [3, 8] 1.38 <new_layer_p> ryy [1, 3] 1.25 <new_layer_p> rxx [7, 11] 0.02 <new_layer_p> rzz [2, 4] 1.36 <new_layer_p> rzz [1, 5] 1.27 <new_layer_p> rzz [1, 6] 3.14 <new_layer_p> rxx [2, 8] 1.28 <new_layer_p> rzz [2, 12] 1.17 <new_layer_p> rzz [4, 5] 1.38 <new_layer_p> ryy [1, 12] 1.39 <new_layer_p> rzz [0, 9] 1.25 <new_layer_p> rzz [0, 12] 1.42 <new_layer_p> rzz [3, 5] 1.28 <new_layer_p> rzz [4, 14] 1.17 <new_layer_p> rxx [3, 7] 1.17 <new_layer_p> ryy [0, 11] 1.29 <new_layer_p> ryy [1, 7] 1.17 <new_layer_p> rxx [0, 3] 1.33 <new_layer_p> ryy [1, 14] 1.31 <new_layer_p> rzz [4, 5] 1.69 <new_layer_p> rzz [3, 7] 1.35 <new_layer_p> rxx [6, 10] 1.33 <new_layer_p> rxx [3, 12] 1.39 <new_layer_p> rzz [2, 14] 1.35 <new_layer_p> rzz [8, 14] 1.32 <new_layer_p> rzz [3, 12] 1.17 <new_layer_p> rzz [1, 11] 1.28 <new_layer_p> rzz [3, 9] 1.41 <new_layer_p> rxx [8, 11] 1.45 <new_layer_p> ryy [11, 13] 1.32 <new_layer_p> ryy [3, 12] 1.36 <new_layer_p> rxx [13, 14] 1.33 <new_layer_p> rzz [1, 5] 1.29 <new_layer_p> rzz [9, 14] 1.45 <new_layer_p> rxx [3, 9] 1.39 <new_layer_p> rzz [1, 3] 1.27 <new_layer_p> rxx [9, 12] 1.32 <new_layer_p> rxx [6, 8] 1.25 <new_layer_p> rxx [9, 11] 1.36 <new_layer_p> rxx [0, 8] 1.28 <new_layer_p> rzz [1, 14] 1.36 <new_layer_p> rxx [9, 14] 1.41 <new_layer_p> rzz [5, 12] 1.16 <new_layer_p> rxx [13, 14] 1.38 <new_layer_p> ryy [1, 7] 1.20 <new_layer_p> rzz [1, 7] 1.69 <new_layer_p> rzz [2, 6] 1.17 <new_layer_p> rzz [1, 5] 1.33 <new_layer_p> rzz [3, 7] 1.33 <new_layer_p> rzz [5, 8] 1.35 <new_layer_p> rzz [1, 3] 1.38 <new_layer_p> rzz [1, 4] 1.33 <new_layer_p> ryy [3, 11] 202.91 <new_layer_p> ryy [4, 5] 1.36 <new_layer_p> rzz [10, 11] 1.43 <new_layer_p> rzz [4, 12] 1.31 <new_layer_p> rzz [2, 9] 1.25 <new_layer_p> rzz [1, 9] 1.31 <new_layer_p> ryy [6, 8] 3.13 <new_layer_p> rzz [0, 12] 1.41 <new_layer_p> rxx [4, 10] 1.21 <new_layer_p> rzz [2, 11] 1.17 <new_layer_p> rzz [1, 7] 1.37 <new_layer_p> rzz [3, 8] 1.39 <new_layer_p> rxx [1, 5] 1.36 <new_layer_p> rzz [4, 11] 1.36 <new_layer_p> ryy [7, 10] 1.27 <new_layer_p> rzz [9, 11] 1.40 <new_layer_p> rzz [9, 14] 1.35 <new_layer_p> rzz [7, 14] 1.36 <new_layer_p> rzz [3, 7] 1.25 <new_layer_p> rxx [8, 11] 1.35 <new_layer_p> ryy [4, 12] 1.33 <new_layer_p> rzz [0, 3] 1.33 <new_layer_p> ryy [3, 7] 200.91 <new_layer_p> rxx [5, 6] 1.36 <new_layer_p> ryy [10, 14] 1.39 <new_layer_p> rxx [1, 5] 1.33 <new_layer_p> rxx [4, 11] 1.25 <new_layer_p> rxx [3, 8] 1.32 <new_layer_p> rzz [3, 13] 1.28 <new_layer_p> rzz [4, 11] 1.33 <new_layer_p> rxx [4, 14] 1.40 <new_layer_p> rxx [4, 10] 1.33 <new_layer_p> rxx [2, 4] 1.37 <new_layer_p> rxx [4, 11] 1.33 <new_layer_p> ryy [10, 14] 1.40 <new_layer_p> rzz [2, 5] 1.45 <new_layer_p> rxx [4, 6] 1.33 <new_layer_p> rzz [6, 10] 1.27 <new_layer_p> rzz [0, 7] 1.01 <new_layer_p> rzz [5, 11] 1.37 <new_layer_p> rzz [11, 13] 1.41 <new_layer_p> rzz [1, 12] 1.32 <new_layer_p> ryy [8, 14] 1.39 <new_layer_p> rzz [6, 8] 1.45 <new_layer_p> rxx [0, 9] 1.25 <new_layer_p> ryy [1, 13] 1.33 <new_layer_p> rzz [6, 10] 1.25 <new_layer_p> rxx [3, 10] 1.33 <new_layer_p> ryy [3, 10] 1.26 <new_layer_p> rxx [10, 12] 1.39 <new_layer_p> ryy [4, 11] 1.44 <new_layer_p> rzz [4, 8] 1.36 <new_layer_p> rzz [2, 11] 1.32 <new_layer_p> rzz [5, 7] 1.33 <new_layer_p> ryy [6, 14] 1.33 <new_layer_p> rzz [5, 8] 1.35 <new_layer_p> ryy [7, 10] 1.47 <new_layer_p> rzz [7, 11] 1.43 <new_layer_p> rzz [1, 10] 1.28 <new_layer_p> rzz [3, 5] 3.14 <new_layer_p> rzz [2, 5] 1.32 <new_layer_p> ryy [5, 8] 1.31 <new_layer_p> rxx [5, 6] 1.31 <new_layer_p> rzz [3, 7] 1.41 <new_layer_p> rzz [0, 4] 1.36 <new_layer_p> rzz [2, 4] 1.17 <new_layer_p> ryy [0, 10] 1.35 <new_layer_p> rxx [2, 9] 1.37 <new_layer_p> ryy [5, 12] 1.27 <new_layer_p> rxx [3, 9] 1.25 <new_layer_p> ryy [5, 11] 1.32 <new_layer_p> ryy [5, 13] 1.36 <new_layer_p> ryy [3, 7] 1.17 <new_layer_p> rzz [0, 10] 1.28 <new_layer_p> rzz [3, 7] 1.33 <new_layer_p> rxx [3, 13] 1.28 <new_layer_p> ryy [2, 5] 1.17 <new_layer_p> rxx [9, 10] 1.32 <new_layer_p> rxx [0, 9] 1.35 <new_layer_p> rxx [10, 11] 11.90 <new_layer_p> rxx [8, 13] 1.17 <new_layer_p> rxx [4, 9] 1.19 <new_layer_p> rzz [4, 7] 1.27 <new_layer_p> rxx [4, 6] 1.33 <new_layer_p> rzz [6, 11] 1.40 <new_layer_p> ryy [8, 14] 1.34 <new_layer_p> rzz [4, 8] 1.32 <new_layer_p> rxx [4, 11] 1.32 <new_layer_p> ryy [2, 6] 3.14 <new_layer_p> ryy [3, 13] 1.25 <new_layer_p> ryy [3, 5] 1.36 <new_layer_p> ryy [2, 11] 1.30 <new_layer_p> rzz [2, 9] 1.17 <new_layer_p> rzz [0, 10] 1.39 <new_layer_p> rxx [3, 12] 1.31 <new_layer_p> ryy [4, 5] 1.36 <new_layer_p> rxx [0, 11] 1.26 <new_layer_p> rxx [2, 13] 1.36 <new_layer_p> ryy [0, 5] 3.14 <new_layer_p> rxx [10, 14] 1.48 <new_layer_p> ryy [5, 12] 1.37 <new_layer_p> rxx [0, 8] 3.14 <new_layer_p> rxx [13, 14] 1.32 <new_layer_p> rxx [3, 13] 1.27 <new_layer_p> rzz [1, 11] 1.28 <new_layer_p> ryy [4, 5] 1.45 <new_layer_p> ryy [4, 10] 1.36 <new_layer_p> ryy [2, 9] 1.43 <new_layer_p> rxx [3, 9] 1.33 <new_layer_p> ryy [0, 1] 1.37 <new_layer_p> rxx [1, 2] 1.38 <new_layer_p> ryy [2, 9] 1.17 <new_layer_p> rzz [0, 10] 1.32 <new_layer_p> rxx [2, 10] 1.36 <new_layer_p> ryy [12, 14] 1.39 <new_layer_p> rxx [3, 9] 1.34 <new_layer_p> rxx [8, 11] 1.36 <new_layer_p> rzz [1, 7] 1.25 <new_layer_p> rzz [0, 8] 1.39 <new_layer_p> rzz [6, 7] 1.30 <new_layer_p> rxx [7, 14] 1.36 <new_layer_p> rzz [6, 8] 1.28 <new_layer_p> rzz [1, 7] 1.38 <new_layer_p> rzz [6, 9] 1.31 <new_layer_p> rzz [1, 3] 1.41 <new_layer_p> rzz [4, 10] 1.36 <new_layer_p> ryy [12, 14] 0.02 <new_layer_p> rzz [10, 11] 1.33 <new_layer_p> ryy [4, 9] 1.35 <new_layer_p> rxx [3, 7] 1.33 <new_layer_p> ryy [11, 13] 208.06 <new_layer_p> rzz [2, 9] 1.39 <new_layer_p> rxx [8, 14] 1.33 <new_layer_p> rxx [5, 13] 1.36 <new_layer_p> rzz [8, 9] 1.39 <new_layer_p> ryy [1, 14] 1.28 <new_layer_p> rzz [6, 10] 1.33 <new_layer_p> rxx [4, 7] 1.38 <new_layer_p> rxx [9, 11] 1.32 <new_layer_p> rxx [1, 3] 1.25 <new_layer_p> rxx [3, 9] 1.32 <new_layer_p> rxx [11, 13] 207.74 <new_layer_p> ryy [5, 11] 1.33 <new_layer_p> rzz [6, 11] 1.16 <new_layer_p> rzz [8, 11] 1.39 <new_layer_p> rzz [8, 11] 1.32 <new_layer_p> rzz [2, 5] 1.32 <new_layer_p> ryy [5, 12] 1.20 <new_layer_p> ryy [3, 12] 1.39 <new_layer_p> rzz [6, 8] 1.36 <new_layer_p> rzz [9, 10] 1.28 <new_layer_p> rxx [8, 14] 1.38 <new_layer_p> rzz [6, 12] 1.33 <new_layer_p> ryy [2, 9] 1.35 <new_layer_p> ryy [2, 14] 1.28 <new_layer_p> rzz [6, 9] 1.19 <new_layer_p> rzz [4, 14] 1.37 <new_layer_p> rxx [0, 12] 1.33 <new_layer_p> ryy [8, 14] 1.32 <new_layer_p> ryy [6, 14] 1.33 <new_layer_p> rxx [0, 1] 1.34 <new_layer_p> rzz [1, 10] 1.28 <new_layer_p> rzz [4, 6] 1.48 <new_layer_p> ryy [2, 4] 1.25 <new_layer_p> rzz [5, 11] 1.21 <new_layer_p> rzz [3, 5] 1.36 <new_layer_p> rzz [6, 10] 1.36 <new_layer_p> ryy [5, 8] 0.74 <new_layer_p> rxx [5, 8] 1.45 <new_layer_p> rzz [0, 13] 1.36 <new_layer_p> rzz [1, 10] 1.60 <new_layer_p> rzz [0, 9] 1.41 <new_layer_p> rxx [2, 8] 1.36 <new_layer_p> rzz [4, 11] 1.17 <new_layer_p> rzz [7, 14] 1.38 <new_layer_p> rzz [0, 2] 1.41 <new_layer_p> ryy [2, 6] 1.17 <new_layer_p> ryy [7, 9] 0.02 <new_layer_p> rzz [3, 14] 1.16 <new_layer_p> rzz [2, 4] 1.27 <new_layer_p> rzz [0, 6] 1.35 <new_layer_p> ryy [6, 10] 1.25 <new_layer_p> rxx [1, 3] 1.36 <new_layer_p> rxx [6, 8] 1.36 <new_layer_p> rzz [10, 14] 1.45 <new_layer_p> ryy [5, 10] 215.76 <new_layer_p> rzz [3, 5] 3.14 <new_layer_p> ryy [9, 11] 1.33 <new_layer_p> rxx [5, 8] 1.17 <new_layer_p> rxx [1, 12] 1.17 <new_layer_p> rzz [7, 13] 1.26 <new_layer_p> ryy [7, 14] 1.33 <new_layer_p> rzz [9, 11] 1.33 <new_layer_p> rxx [0, 9] 1.29 <new_layer_p> rzz [6, 14] 1.33 <new_layer_p> rxx [2, 11] 1.28 <new_layer_p> rzz [2, 13] 1.38 <new_layer_p> rzz [8, 11] 1.32 <new_layer_p> rxx [3, 5] 1.22 <new_layer_p> rzz [0, 4] 1.32 <new_layer_p> rzz [10, 13] 1.27 <new_layer_p> rxx [5, 9] 1.27 <new_layer_p> rzz [8, 14] 1.25 <new_layer_p> rxx [3, 13] 1.29 <new_layer_p> ryy [2, 3] 1.27 <new_layer_p> ryy [3, 5] 1.37 <new_layer_p> rzz [2, 9] 1.21 <new_layer_p> rxx [0, 11] 1.31 <new_layer_p> rzz [5, 6] 1.43 <new_layer_p> ryy [8, 13] 1.28 <new_layer_p> rxx [0, 1] 1.25 <new_layer_p> rzz [7, 13] 1.55 <new_layer_p> rzz [5, 8] 1.28 <new_layer_p> rzz [3, 14] 1.35 <new_layer_p> rxx [4, 11] 1.32 <new_layer_p> rxx [5, 13] 1.17 <new_layer_p> ryy [1, 12] 1.32 <new_layer_p> rxx [5, 8] 1.33 <new_layer_p> rxx [4, 5] 1.10 <new_layer_p> rzz [1, 12] 1.32 <new_layer_p> rzz [3, 14] 1.17 <new_layer_p> rxx [0, 7] 1.28 <new_layer_p> ryy [9, 11] 1.40 <new_layer_p> rzz [9, 11] 1.28 <new_layer_p> ryy [4, 6] 1.37 <new_layer_p> rzz [12, 14] 1.25 <new_layer_p> rzz [4, 11] 1.35 <new_layer_p> ryy [5, 7] 1.17 <new_layer_p> rzz [11, 12] 1.28 <new_layer_p> rxx [2, 3] 1.28 <new_layer_p> rxx [9, 10] 1.14 <new_layer_p> rzz [4, 12] 1.33 <new_layer_p> rxx [2, 8] 1.39 <new_layer_p> rzz [2, 5] 1.30 <new_layer_p> rzz [2, 10] 1.33 <new_layer_p> rzz [10, 11] 1.22 <new_layer_p> rzz [1, 5] 1.33 <new_layer_p> ryy [6, 8] 1.28 <new_layer_p> rxx [1, 7] 1.35 <new_layer_p> rzz [2, 9] 1.38 <new_layer_p> rzz [6, 7] 1.27 <new_layer_p> rxx [4, 14] 1.32 <new_layer_p> rzz [0, 8] 1.23 <new_layer_p> rxx [4, 10] 1.28 <new_layer_p> rzz [8, 9] 1.33 <new_layer_p> rzz [5, 8] 1.28 <new_layer_p> rxx [6, 7] 1.17 <new_layer_p> rzz [10, 11] 1.33 <new_layer_p> rzz [1, 4] 1.33 <new_layer_p> rzz [0, 10] 1.47 <new_layer_p> rxx [6, 7] 1.30 <new_layer_p> rxx [6, 14] 1.49 <new_layer_p> rxx [0, 4] 1.30 <new_layer_p> rxx [7, 8] 1.26 <new_layer_p> rzz [3, 4] 1.33 <new_layer_p> rzz [3, 9] 1.28 <new_layer_p> rzz [3, 10] 1.32 <new_layer_p> ryy [8, 9] 198.11 <new_layer_p> rzz [4, 11] 1.36 <new_layer_p> rxx [4, 7] 209.13 <new_layer_p> rxx [5, 13] 1.27 <new_layer_p> rzz [8, 13] 1.20 <new_layer_p> rzz [7, 8] 1.30 <new_layer_p> rxx [10, 11] 1.46 <new_layer_p> rzz [2, 14] 1.35 <new_layer_p> ryy [2, 4] 1.17 <new_layer_p> rzz [7, 11] 1.20 <new_layer_p> rzz [3, 10] 1.32 <new_layer_p> rzz [8, 10] 1.32 <new_layer_p> rxx [6, 12] 1.36 <new_layer_p> rzz [6, 14] 1.41 <new_layer_p> rzz [1, 7] 1.35 <new_layer_p> ryy [4, 5] 1.32 <new_layer_p> ryy [12, 14] 1.39 <new_layer_p> ryy [7, 8] 1.22 <new_layer_p> ryy [2, 9] 1.38 <new_layer_p> rzz [2, 5] 209.76 <new_layer_p> rxx [3, 7] 1.27 <new_layer_p> rzz [2, 6] 1.34 <new_layer_p> ryy [4, 14] 1.33 <new_layer_p> ryy [1, 13] 0.01 <new_layer_p> rzz [1, 5] 1.46 <new_layer_p> rzz [3, 10] 1.38 <new_layer_p> rzz [2, 11] 1.45 <new_layer_p> rzz [7, 8] 1.17 <new_layer_p> rzz [3, 4] 1.36 <new_layer_p> rxx [13, 14] 1.33 <new_layer_p> rzz [0, 4] 1.27 <new_layer_p> rzz [6, 9] 1.27 <new_layer_p> rzz [3, 8] 1.32 <new_layer_p> ryy [0, 6] 1.32 <new_layer_p> ryy [1, 7] 1.25 <new_layer_p> ryy [3, 4] 1.36 <new_layer_p> rxx [0, 6] 1.39 <new_layer_p> rxx [2, 12] 1.33 <new_layer_p> rxx [2, 11] 1.17 <new_layer_p> rxx [4, 10] 1.32 <new_layer_p> ryy [1, 9] 1.37 <new_layer_p> rzz [1, 12] 1.40 <new_layer_p> rxx [3, 13] 1.33 <new_layer_p> rxx [12, 14] 1.39 <new_layer_p> rxx [5, 9] 1.28 <new_layer_p> rxx [2, 10] 1.19 <new_layer_p> rxx [2, 6] 1.20 <new_layer_p> rzz [3, 7] 1.38 <new_layer_p> rzz [2, 7] 1.25 <new_layer_p> ryy [2, 11] 1.35 <new_layer_p> rzz [7, 14] 1.40 <new_layer_p> rxx [1, 7] 1.33 <new_layer_p> rzz [7, 9] 1.28 <new_layer_p> rxx [1, 2] 1.28 <new_layer_p> rxx [4, 10] 1.33 <new_layer_p> rzz [7, 14] 1.24 <new_layer_p> ryy [0, 2] 1.32 <new_layer_p> rzz [1, 5] 0.01 <new_layer_p> rxx [0, 6] 1.28 <new_layer_p> rzz [5, 6] 1.34 <new_layer_p> ryy [6, 10] 1.25 <new_layer_p> ryy [7, 9] 1.38 <new_layer_p> rxx [3, 12] 1.28 <new_layer_p> rzz [1, 5] 1.36 <new_layer_p> ryy [2, 11] 1.50 <new_layer_p> rzz [6, 9] 1.35 <new_layer_p> rzz [4, 13] 1.45 <new_layer_p> rzz [1, 7] 1.29 <new_layer_p> ryy [3, 9] 1.32 <new_layer_p> rxx [4, 8] 1.33 <new_layer_p> ryy [3, 10] 1.33 <new_layer_p> ryy [10, 13] 1.23 <new_layer_p> rxx [1, 7] 1.28 <new_layer_p> ryy [2, 12] 1.40 <new_layer_p> rzz [5, 6] 1.39 <new_layer_p> ryy [3, 11] 1.28 <new_layer_p> ryy [0, 14] 1.18 <new_layer_p> ryy [9, 11] 1.35 <new_layer_p> ryy [2, 3] 0.33 (6,12) 0.65 <new_layer_p> rxx [3, 7] 1.28 <new_layer_p> rzz [1, 9] 1.27 <new_layer_p> rxx [4, 12] 1.36 <new_layer_p> rzz [8, 11] 1.39 <new_layer_p> rzz [8, 14] 1.38 <new_layer_p> rzz [4, 9] 1.27 <new_layer_p> ryy [2, 14] 1.28 <new_layer_p> ryy [8, 12] 1.28 <new_layer_p> rzz [3, 12] 1.33 <new_layer_p> rxx [9, 12] 1.33 <new_layer_p> rzz [2, 7] 1.33 <new_layer_p> rxx [1, 13] 1.36 <new_layer_p> rxx [3, 6] 1.34 <new_layer_p> rzz [7, 12] 1.33 <new_layer_p> rzz [1, 2] 1.43 <new_layer_p> rzz [3, 7] 1.40 <new_layer_p> rxx [6, 10] 1.39 <new_layer_p> rzz [1, 14] 1.25 <new_layer_p> rxx [1, 10] 1.33 <new_layer_p> rxx [5, 8] 1.35 <new_layer_p> rzz [1, 11] 1.33 <new_layer_p> rzz [9, 11] 1.39 <new_layer_p> rzz [0, 10] 1.39 <new_layer_p> rxx [6, 14] 1.34 <new_layer_p> rzz [1, 9] 1.32 <new_layer_p> rxx [1, 6] 1.17 <new_layer_p> rzz [9, 11] 1.27 <new_layer_p> rxx [1, 9] 1.32 <new_layer_p> rzz [0, 13] 198.40 <new_layer_p> rxx [8, 9] 1.33 <new_layer_p> ryy [3, 13] 1.01 <new_layer_p> rxx [7, 12] 1.25 <new_layer_p> rzz [3, 10] 1.40 <new_layer_p> ryy [9, 11] 1.38 <new_layer_p> rxx [10, 13] 1.36 <new_layer_p> rxx [2, 12] 1.19 <new_layer_p> rzz [0, 9] 0.01 <new_layer_p> rzz [5, 8] 1.17 <new_layer_p> rzz [11, 12] 1.12 <new_layer_p> rxx [4, 9] 1.33 <new_layer_p> ryy [6, 14] 1.16 <new_layer_p> ryy [2, 6] 1.34 <new_layer_p> rzz [0, 8] 1.27 <new_layer_p> rxx [2, 9] 1.33 <new_layer_p> rzz [0, 8] 1.36 <new_layer_p> ryy [5, 11] 3.14 <new_layer_p> rzz [3, 7] 1.33 <new_layer_p> rxx [2, 9] 1.38 <new_layer_p> ryy [2, 10] 198.40 <new_layer_p> rxx [1, 9] 1.45 <new_layer_p> rzz [2, 7] 0.01 <new_layer_p> rzz [12, 13] 213.85 <new_layer_p> rzz [2, 10] 1.45 <new_layer_p> ryy [2, 9] 1.33 <new_layer_p> rxx [0, 1] 1.41 <new_layer_p> rzz [8, 11] 1.34 <new_layer_p> ryy [5, 7] 1.17 <new_layer_p> rzz [2, 13] 1.28 <new_layer_p> rxx [4, 8] 1.37 <new_layer_p> rzz [3, 8] 1.28 <new_layer_p> rzz [6, 9] 1.19 <new_layer_p> rxx [7, 13] 1.35 <new_layer_p> rzz [6, 10] 1.27 <new_layer_p> rzz [7, 10] 1.27 <new_layer_p> rzz [3, 6] 1.38 <new_layer_p> rzz [3, 5] 1.36 <new_layer_p> rzz [1, 8] 1.27 <new_layer_p> rxx [0, 6] 1.39 <new_layer_p> rxx [2, 7] 0.02 <new_layer_p> rzz [0, 1] 1.33 <new_layer_p> rxx [9, 14] 1.41 <new_layer_p> rxx [2, 8] 188.63 <new_layer_p> rzz [4, 5] 1.43 <new_layer_p> rzz [8, 13] 1.28 <new_layer_p> rxx [2, 7] 1.31 <new_layer_p> ryy [0, 9] 1.41 <new_layer_p> ryy [0, 5] 1.28 <new_layer_p> rzz [7, 9] 1.31 <new_layer_p> ryy [1, 3] 1.37 <new_layer_p> ryy [1, 14] 1.32 <new_layer_p> rzz [3, 14] 1.32 <new_layer_p> rxx [2, 5] 1.17 <new_layer_p> rzz [3, 7] 1.40 <new_layer_p> rxx [2, 9] 1.38 <new_layer_p> rxx [4, 12] 1.33\n"
     ]
    }
   ],
   "source": [
    "start_t = time.time()\n",
    "if device == \"cuda\":\n",
    "    print(\"cuda\")\n",
    "\n",
    "    # GPU\n",
    "    # generated_circuit_tokens = generate_long_circuit_compiled(\n",
    "    #     model,\n",
    "    #     gptconf,\n",
    "    #     graph_tokens,\n",
    "    #     stoi,\n",
    "    #     itos,\n",
    "    #     max_total_tokens=7000,\n",
    "    #     temperature=0.5,\n",
    "    #     top_k=None\n",
    "    # )\n",
    "\n",
    "    # GPU - cached\n",
    "    generated_circuit_tokens = generate_long_circuit_compiled_cached(\n",
    "        model,\n",
    "        gptconf,\n",
    "        graph_tokens,\n",
    "        stoi,\n",
    "        itos,\n",
    "        max_total_tokens=7000, # 15 nodes graph and format v1\n",
    "        # max_total_tokens=12000, # 15 nodes graph and format v2\n",
    "        temperature=0.5,\n",
    "        top_k=None\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"cpu\")\n",
    "\n",
    "    # CPU\n",
    "    # generated_circuit_tokens = generate_long_circuit(\n",
    "    #     model,\n",
    "    #     graph_tokens,\n",
    "    #     stoi,\n",
    "    #     itos,\n",
    "    #     max_total_tokens=7000,\n",
    "    #     temperature=0.5,\n",
    "    #     top_k=None\n",
    "    # )\n",
    "\n",
    "    # CPU - cached\n",
    "    generated_circuit_tokens = generate_long_circuit_cpu(\n",
    "        model,\n",
    "        gptconf,\n",
    "        graph_tokens,\n",
    "        stoi,\n",
    "        itos,\n",
    "        max_total_tokens=7000, # 15 nodes graph and format v1\n",
    "        # max_total_tokens=12000, # 15 nodes graph and format v2\n",
    "        temperature=0.5,\n",
    "        top_k=None\n",
    "    )\n",
    "    \n",
    "end_t = time.time()\n",
    "\n",
    "print(f\"Generation time: {end_t - start_t} secs\")\n",
    "\n",
    "print(f\"Generated QAOA Circuit (length: {len(generated_circuit_tokens)}):\")\n",
    "print(\" \".join(generated_circuit_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_of_graph = 0\n",
    "brackets = 0\n",
    "for tok in generated_circuit_tokens:\n",
    "    if tok == \"<end_of_graph>\":\n",
    "        end_of_graph += 1\n",
    "    elif tok[0] == \"(\":\n",
    "        brackets += 1\n",
    "\n",
    "end_of_graph, brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6788"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "generated_circuit_tokens_cached = copy.deepcopy(generated_circuit_tokens)\n",
    "len(generated_circuit_tokens_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<new_layer_p>',\n",
       " 'rz',\n",
       " '[13]',\n",
       " '-9.29',\n",
       " '<new_layer_p>',\n",
       " 'rz',\n",
       " '[14]',\n",
       " '207.99',\n",
       " '<new_layer_p>',\n",
       " 'rzz',\n",
       " '[11, 12]',\n",
       " '0.25',\n",
       " '(11,14)',\n",
       " '0.70',\n",
       " '(11,14)',\n",
       " '0.61',\n",
       " '(4,7)',\n",
       " '0.82',\n",
       " '<end_of_graph>',\n",
       " '<new_layer_p>']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_circuit_tokens_cached[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6788"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "generated_circuit_tokens_not_cached = copy.deepcopy(generated_circuit_tokens)\n",
    "len(generated_circuit_tokens_not_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<new_layer_p>',\n",
       " 'rz',\n",
       " '[0]',\n",
       " '-9.51',\n",
       " '<new_layer_p>',\n",
       " 'rz',\n",
       " '[1]',\n",
       " '-9.45',\n",
       " '<new_layer_p>',\n",
       " 'rz',\n",
       " '[2]',\n",
       " '-9.54',\n",
       " '<new_layer_p>',\n",
       " 'rz',\n",
       " '[3]',\n",
       " '-9.51',\n",
       " '<new_layer_p>',\n",
       " 'rz',\n",
       " '[4]',\n",
       " '-9.54']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_circuit_tokens_not_cached[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char accuracy: 0.061121782558586246\n",
      "Token accuracy: 0.04286977018267531\n",
      "Levenshtein distance: 12539, Normalized distance: 0.23857452718901023\n"
     ]
    }
   ],
   "source": [
    "limit_tok = min(len(generated_circuit_tokens_not_cached), len(generated_circuit_tokens_cached))\n",
    "\n",
    "generated_circuit_str_not_cached = \" \".join(generated_circuit_tokens_not_cached)\n",
    "generated_circuit_str_cached = \" \".join(generated_circuit_tokens_cached)\n",
    "\n",
    "print(\"Char accuracy:\", char_accuracy(generated_circuit_str_not_cached, generated_circuit_str_cached))\n",
    "print(\"Token accuracy:\", token_accuracy(generated_circuit_str_cached[:limit_tok], generated_circuit_str_not_cached[:limit_tok]))\n",
    "\n",
    "distance, norm_distance = levenshtein_distance(generated_circuit_str_not_cached, generated_circuit_str_cached)\n",
    "print(f\"Levenshtein distance: {distance}, Normalized distance: {norm_distance}\")\n",
    "\n",
    "# # Tokenize the strings\n",
    "references = [circuit_tokens[:limit_tok]] # list of references\n",
    "candidate = generated_circuit_tokens[:limit_tok]\n",
    "print(f\"BLEU score: {bleu_score(references, candidate)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences lenghts (str): 44000\n",
      "Sequences lenghts (tok): 6788\n"
     ]
    }
   ],
   "source": [
    "limit_str = 44000\n",
    "limit_tok = 5500\n",
    "\n",
    "generated_circuit_str = \" \".join(generated_circuit_tokens)\n",
    "generated_circuit_str = generated_circuit_str[:limit_str]\n",
    "\n",
    "print(f\"Sequences lenghts (str): {len(generated_circuit_str)}\")\n",
    "print(f\"Sequences lenghts (tok): {len(generated_circuit_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generated result evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences lenghts (str): (46297, 50876)\n",
      "Sequences lenghts (tok): (6770, 6601)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(44000, 44000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit_str = 44000\n",
    "limit_tok = 5500\n",
    "generated_circuit_str = \" \".join(generated_circuit_tokens)\n",
    "ground_truth_circuit_str = \" \".join(circuit_tokens)\n",
    "\n",
    "print(f\"Sequences lenghts (str): {len(generated_circuit_str), len(ground_truth_circuit_str)}\")\n",
    "print(f\"Sequences lenghts (tok): {len(generated_circuit_tokens), len(circuit_tokens)}\")\n",
    "\n",
    "generated_circuit_str = generated_circuit_str[:limit_str]\n",
    "ground_truth_circuit_str = ground_truth_circuit_str[:limit_str]\n",
    "\n",
    "len(generated_circuit_str), len(ground_truth_circuit_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char accuracy: 0.05402272727272727\n",
      "Token accuracy: 0.1410909090909091\n",
      "Levenshtein distance: 16774, Normalized distance: 0.38122727272727275\n",
      "BLEU score: 0.027566233560436897\n"
     ]
    }
   ],
   "source": [
    "print(\"Char accuracy:\", char_accuracy(generated_circuit_str, ground_truth_circuit_str))\n",
    "print(\"Token accuracy:\", token_accuracy(generated_circuit_tokens[:limit_tok], circuit_tokens[:limit_tok]))\n",
    "\n",
    "distance, norm_distance = levenshtein_distance(generated_circuit_str, ground_truth_circuit_str)\n",
    "print(f\"Levenshtein distance: {distance}, Normalized distance: {norm_distance}\")\n",
    "\n",
    "# Tokenize the strings\n",
    "references = [circuit_tokens[:limit_tok]] # list of references\n",
    "candidate = generated_circuit_tokens[:limit_tok]\n",
    "print(f\"BLEU score: {bleu_score(references, candidate)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save generated and groud trouth sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(generated_circuit_tokens), len(circuit_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generated_circuit_tokens_2.pkl'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_gen = f\"generated_circuit_tokens_{sample_idx:d}.pkl\"\n",
    "# filename_gt = f\"circuit_tokens_{sample_idx:3d}.pkl\"\n",
    "\n",
    "filename_gen#, filename_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generated sequence\n",
    "save_seq_tokens_to_file(generated_circuit_tokens, filename_gen)\n",
    "generated_circuit_tokens_loaded = load_seq_tokens_from_file(filename_gen)\n",
    "any([e1 == e2 for e1, e2 in zip(generated_circuit_tokens, generated_circuit_tokens_loaded)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ground truth sequence\n",
    "# save_seq_tokens_to_file(circuit_tokens, filename_gt)\n",
    "# circuit_tokens_loaded = load_seq_tokens_from_file(filename_gt)\n",
    "# any([e1 == e2 for e1, e2 in zip(circuit_tokens, circuit_tokens_loaded)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12175"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [Default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "1cf486887112cd4e502a2e435ae8b81b4422a2c251582405ce1e7c1278da5bff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
